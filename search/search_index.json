{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> DEEL With Augmentation is a repository which implements several ML techniques to generatively augment the data and that analyzes those methods from a Fairness perspective. </p> <p>This library includes two types of ML approaches to generate synthetic data to augment your training dataset: Generative Adversarial Networks (GANs) and Style Transfer (ST).</p> <p> </p> <p>The main focus of this work is to analyze the ability of those methods to efficiently adress bias issues. More specifically, we wanted to check that augmenting a minority group (i.e. a group that is under represented) with those approaches could alleviate the drop in performance for the sensitive group that is usually observable when training a model on the unaugmented dataset. We made a number of experiments on the EuroSat dataset where the minority group is the satellite images with a blue veil and on the CelebA dataset on which multiple minority groups could be defined.</p>"},{"location":"#experiment-results","title":"\ud83e\uddea Experiment Results","text":"<ul> <li>DEEL With Eurosat Bias experiment's summary</li> <li>DEEL With CelebA Bias experiment's summary</li> </ul> Experiments on Eurosat Bias in EuroSAT Notebook Summary Fourier Domain Adaptation (FDA) Style Flow Contrastive Coherence Preserving Loss for Versatile Style Transfer (CCPL) Adaptive Instance Normalization (AdaIN) Neural Neighbor Style Transfer (NNST) GAN + ST XAI Attributions XAI Metrics <p> <p> <p> An example of a case where we used FDA to change the style of highway images. </p></p></p> Experiments on CelebA Bias in CelebA Notebook Summary OpenCV DCGAN CycleGAN ProGAN All methods (including DDPM) <p> <p> <p> An example of a case where we used CycleGAN to change the hair color of male. </p></p></p>"},{"location":"#the-augmentation-package","title":"\ud83d\udc0d The Augmentation Package","text":"<p>In this repository we packaged all the code used for our experiments to allow both: reproducibility and usability as a Python package of the augmentation methods.</p> Getting Started <p>Augmentare requires a version of python higher than 3.7 and several libraries including Pytorch and Numpy. Installation can be done using:</p> <pre><code>git clone https://github.com/deel-ai/augmentare.git\ncd augmentare\npython setup.py install\n</code></pre> <p>Now that Augmentare is installed.</p> <p>Generative Adversarial Networks Methods</p> <pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\nfrom augmentare.plots.plot_losses import plot_losses_gan\nfrom augmentare.plots.plot_samples import plot_image\n</code></pre> <p>All GANs models share a common API. You can find out more about it here.</p> <p>Style Transfer Methods</p> <pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\nfrom augmentare.plots.plot_losses import plot_losses\n</code></pre> <p>All NST models share a common API. You can find out more about it here</p> Generative Adversarial Networks <p>The library includes a <code>gan</code> module where various GAN models are available. They all come with explanations, tutorials, and links to official articles:</p> GAN Method Source Tutorial Deep Convolutional GAN (DCGAN) Paper Conditional GAN (CGAN) Paper Conditional Deep Convolutional GAN (CDCGAN) Paper Cycle GAN Paper Progressive Growing of GANS (ProGAN) Paper Style Transfer <p>The library includes a <code>style_transfer</code> module where various Style Transfer approaches are available. They all come with explanations, tutorials, and links to official articles:</p> ST Method Source Tutorial Fourrier Domain Adaptation (FDA) Paper Adaptive Instance Normalization (AdaIN) Paper Neural Neighbor Style Transfer (NNST) Paper Style Flow Paper Contrastive Coherence Preserving Loss for Versatile Style Transfer (CCPL) Paper"},{"location":"#see-also","title":"\ud83d\udc40 See Also","text":"<p>More from the DEEL project:</p> <ul> <li>Xplique a Python library exclusively dedicated to explaining neural networks.</li> <li>deel-lip a Python library for training k-Lipschitz neural networks on TF.</li> <li>deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch.</li> <li>Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset.</li> <li>LARD Landing Approach Runway Detection (LARD) is a dataset of aerial front view images of runways designed for aircraft landing phase</li> <li>PUNCC Puncc (Predictive uncertainty calibration and conformalization) is an open-source Python library that integrates a collection of state-of-the-art conformal prediction algorithms and related techniques for regression and classification problems</li> <li>OODEEL OODeel is a library that performs post-hoc deep OOD detection on already trained neural network image classifiers. The philosophy of the library is to favor quality over quantity and to foster easy adoption</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>  This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the  DEEL  project.</p>"},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>This repository was developed by Vuong NGUYEN as part of his apprenticeship with the  DEEL  Team under the supervision of Lucas Hervier and Agustin PICARD. He is currently a student in dual engineering degree ModIA program at INSA Toulouse and INP-ENSEEIHT supported by Artificial and Natural Intelligence Toulouse Institute (ANITI).</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under  MIT license.</p>"},{"location":"api/experiments_summary/Bias_CelebA/","title":"Summary","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom PIL import Image as PilImage\nfrom IPython.display import HTML\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom utils_plot import plot_res_celeba_global, plot_best_res_celeba, plot_res_celeba_minority\n</code></pre> <pre><code>df_attr = pd.read_csv('/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/dataset/list_attr_celeba.csv')\n</code></pre> <pre><code>df_attr.replace(-1,0,inplace=True)\ndata = df_attr[['image_id', 'Male', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']]\n\ndata_male = data[data[\"Male\"]==1]\ndata_female = data[data[\"Male\"]==0]\n\narr=pd.DataFrame(data_male.iloc[:,2:].sum(axis=0))\narr.columns=['Male']\n\narr1=pd.DataFrame(data_female.iloc[:,2:].sum(axis=0))\narr1.columns=['Female']\nresult = pd.concat([arr, arr1], axis=1)\n</code></pre> <pre><code>fig = go.Figure()\nx = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']\n\nfig.add_trace(go.Bar(\n    name='Male',\n    x=x, y=result.Male))\n\nfig.add_trace(go.Bar(\n    name='Female',\n    x=x, y=result.Female))\n\nfig.update_layout(title=\"Number of men and women on different hair colors in raw dataset\", yaxis_title=\"Number\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> We can see a big difference in the number of male and female images in blond and gray hair colors in the raw dataset. For example, the blond female has 28234 images which is very large compared to the blond male having 1749 images, which is 93.8% smaller. And the gray male with 7235 images is quite large compared to the gray female having 1264 images, which is 82.52% smaller. This shows that the blond hair male and the gray hair female account for a very small percentage of the CelebA dataset. </p> <pre><code>train_df = pd.read_csv('/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/dataframe_dataset/train_df.csv')\ndata = train_df\n</code></pre> <pre><code>data_male = data[data[\"Male\"]==1]\ndata_female = data[data[\"Male\"]==0]\n\narr=pd.DataFrame(data_male.iloc[:,2:6].sum(axis=0))\narr.columns=['Male']\n\narr1=pd.DataFrame(data_female.iloc[:,2:6].sum(axis=0))\narr1.columns=['Female']\nresult = pd.concat([arr, arr1], axis=1)\n</code></pre> <pre><code>fig = go.Figure()\nx = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']\n\nfig.add_trace(go.Bar(\n    name='Male',\n    x=x, y=result.Male))\n\nfig.add_trace(go.Bar(\n    name='Female',\n    x=x, y=result.Female))\n\nfig.update_layout(title=\"Number of men and women on different hair colors in train dataset\", yaxis_title=\"Number\", barmode=\"group\")\n# fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> We clearly see that this new train set presents the same setting (significant disparity in the number of male and female in blond and gray hair colors) as the raw dataset, thus we can use it to keep on our experiments. </p> <pre><code>error_rate_baseline = np.load(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_ResNet18/Baseline/error_rate_seed.npy\")\nerr_rate = error_rate_baseline\n</code></pre> <pre><code>fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Combination\", \"Hair Color\", \"Sex\"))\nx = ['Black', 'Blond', 'Brown', 'Gray']\nx_sex = ['Male', 'Female']\n\nfig.add_trace(go.Bar(\n    name='Male',\n    x=x, y=err_rate[:4]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Female',\n    x=x, y=err_rate[4:8]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name=\"Hair Color\",\n    x=x, y=err_rate[8:12]), row=1, col=2)\n\nfig.add_trace(go.Bar(\n    name=\"Sex\",\n    x=x_sex, y=err_rate[12:]), row=1, col=3)\n\nfig.update_layout(title=\"Error rate of image groups (Baseline)\", yaxis_title=\"Error rate\", barmode=\"group\")\n# fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> We find that minority image groups such as blond male and gray female have a significantly higher error rate than their gray male and blond female counterparts. This is exactly as we would expect, minority groups suffer from a significant error rate bias. </p>"},{"location":"api/experiments_summary/Bias_CelebA/#bias-in-celeba","title":"Bias in CelebA","text":"<p> Author - Vuong NGUYEN </p> <ul> <li> The CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. </li> <li> Since we'll be placing ourselves in the fairness framework, we're going to attack the Male/Female binary classification problem, in which we'll explore bias based on celebrity hair color. As we will demonstrate later, this group of images constitutes a discriminatory group in the sense of the equality of errors. We will calculate some statistics about the dataset in the next section. </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#imports","title":"Imports","text":"<p> We import the necessary libraries and tools. </p>"},{"location":"api/experiments_summary/Bias_CelebA/#exploring-celeba","title":"Exploring CelebA","text":""},{"location":"api/experiments_summary/Bias_CelebA/#raw-dataset","title":"Raw dataset","text":"<p> Let's take a look at the distribution of the number of male and female based on different hair colors present in the raw dataset. </p>"},{"location":"api/experiments_summary/Bias_CelebA/#train-dataset","title":"Train dataset","text":"<p> We remove the hairless cases in the raw dataset and split the images into train, test, and validation sets at 70%, 15%, and 15% respectively. Let's see if this new train set exhibits the same bias as the raw dataset. </p> <ul> <li> For specific steps to process the dataset please see the the tutorial here:   View colab tutorial  </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#baseline-resnet18","title":"Baseline (ResNet18)","text":"<p> To evaluate if the presence of the minority groups highlighted previously creates a bias in the learning of an AI model we take a ResNet18 model as a baseline model for comparison of error rates between groups. </p> <ul> <li> For specific steps to make this work with ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source  </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#data-augmentation","title":"Data Augmentation","text":"<p> So the question is how to handle this bias. One way to solve this problem is to implement Data Augmentation methods by creating new images of the minority groups, then adding the augmented images to the original dataset and retraining the model. In this work, we investigated several Data Augmentation approaches to evaluate their ability to mitigate the bias. Next subsections introduce the different methods we used on the CelebA dataset.</p>"},{"location":"api/experiments_summary/Bias_CelebA/#oversampling","title":"Oversampling","text":"<ul> <li> <p> Oversampling is a technique in machine learning used to address class imbalance. It involves increasing the number of instances in the minority groups by generating synthetic samples or replicating existing ones. This aims to balance the distribution of classes in the training dataset, which can improve the performance of machine learning models, particularly in binary classification tasks. </p> </li> <li> <p> In this case, we use the Oversampling method to replicate the images of minorities <code>(blond hair male and gray hair female)</code> into the original dataset. We will replicate the images of the minority group into the original dataset until this minority group reaches the percentage of 5%, 10%,... compared to the total number of images of the class to be predicted <code>(male and female)</code> in the dataset for our task. In summary, Oversampling increases the representativeness of the minority group like <code>blond hair male and gray hair female</code>. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#undersampling","title":"Undersampling","text":"<ul> <li> <p> Undersampling is another technique used to mitigate class imbalance in machine learning datasets. It focuses on reducing the number of instances in the majority group to balance it with the minority group. This is typically done by randomly removing examples from the majority group. Undersampling can help prevent models from being biased toward the majority group and can improve their ability to recognize the minority group. </p> </li> <li> <p> In this case, we use the Undersampling method to randomly removing the images of the majority group <code>(male and female)</code> from the original dataset until the minority group <code>(blond hair male and gray hair female)</code> reach the ratio 5%, 10%,... compared to the total number of images of the class to be predicted <code>(male and female)</code> in the dataset for our task. In summary, Oversampling reduces the representativeness of the majority group. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#change-hair-color","title":"Change hair color","text":"<p> Another approach is to create new images of minorities from images of the majority. In this family of methods we focused on changing the hair's color of an individual from a majority group into a hair's color so this individual belong to the minority group. Specifically, here we focus on transforming:     <ul> <li>  gray hair male to blond hair male  </li> <li>  blond hair female to gray hair female  </li> <p>  We mainly used the following methods:     <ul> <li>  Traditional method is supported by OpenCV tools  </li> <li>  CUT model  </li> <li>  CycleGAN model  </li> <li>  Diffusion model  </li> <p> </p> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Classic/male_blond_classic.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Classic method by OpenCV\")\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Classic/female_gray_classic.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Classic method by OpenCV\")\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We see that different alpha values give different level of hair colors. It depends on the hair color that we want to change will have different appropriate alpha values. For example for blond hair color we choose <code>alpha=0.842</code> and for gray hair color we choose <code>alpha=0.408</code> to create new images. </p> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Hair color/male_blond.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"CUT + CycleGAN\")\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Hair color/female_gray.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"CUT + CycleGAN\")\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We see that all models work well and produce good quality images. JoliGEN's CUT and CycleGAN models sometimes change the face, not just the hair color. Augmentare's CycleGAN model only changes hair color. </p> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Diffusion/diffusion_hair_color.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Diffusion Model\")\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We found that the DDPM model worked well and produced high quality images. However, the problem here is that it is difficult to control the hair color generated, so we temporarily do not use it for our experiments. </p> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Face/male_blond.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"CUT + CycleGAN\")\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Face/female_gray.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"CUT + CycleGAN\")\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We found that all models performed well and produced good-quality images. JoliGEN's CUT and CycleGAN models did a great job in converting the gender of the individual in the image, in fact, it added the desired gender properties such as makeup, lips... The CycleGAN model of Augmentare doesn't work well in this case. Therefore we will use CUT and CycleGAN models of JoliGEN to perform the face transformation task. </p>"},{"location":"api/experiments_summary/Bias_CelebA/#classic-traditional-method-is-supported-by-opencv-tools","title":"Classic (Traditional method is supported by OpenCV tools)","text":"<ul> <li> <p> One approach consist of modifying the hair color of an individual solely based on \"traditional\" computer vision's tools. In this step, we used the tools available in the OpenCV library. In order to do this we will use the hair segmentation technique to get the hair mask from the image of the characters and then change the hair color with the help of the hair mask. To do hair segmentation we use the Mediapipe library, and to change hair color first we have to define the RGB index of color we want, then we use <code>cv2.threshold</code> and <code>cv2. addWeighted</code> functions of OpenCV to do it. Also, we can use an alpha variable to control the level of hair color change in the function <code>cv2.addWeighted</code>. Let's see some examples below. </p> </li> <li> <p> For specific steps to make this work, please see the tutorial here:   View colab tutorial  </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#cut-cyclegan","title":"CUT + CycleGAN","text":"<p> Another way to do hair color change is to use CUT and CycleGAN models: </p> <ul> <li>  CUT:   Contrastive Learning for Unpaired Image-to-Image Translation is a technique in the field of computer vision and image processing. This technique focuses on addressing the problem of unpaired image-to-image translation, which involves converting an image from one domain to another without having a one-to-one correspondence between the images in the source and target domains. The key idea behind contrastive learning in this context is to learn a shared representation space for images from different domains (e.g., photos and paintings) such that similar images from different domains are close together in this space, while dissimilar images are far apart. This learned representation can then be used to perform image translation between the domains, even when you don't have exact pairs of corresponding images.  </li> <li>  CycleGAN:   Cycle-Consistent Adversarial Networks is a type of deep learning model and framework used for unsupervised image-to-image translation. CycleGAN is designed for scenarios where you have two sets of images from different domains and you want to learn a mapping between them without requiring paired examples. In other words, it enables you to perform transformations between two domains without the need for one-to-one correspondence between images in those domains.  </li> </ul> <p> Here we use the CUT and CycleGAN models from the JoliGEN library and the CycleGAN model that we implemented in Augmentare. We need to create a dataset of two folders, one containing the images with the hair color we want to change (trainA) and another containing the images with the hair color we are aiming for ( trainB). </p> <ul> <li> <p> For specific steps to make this work with CUT model (JoliGEN), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> <li> <p> For specific steps to make this work with the CycleGAN model (JoliGEN), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> <li> <p> For specific steps to make this work with the CycleGAN model (Augmentare), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#diffusion-model","title":"Diffusion model","text":"<p> Another interesting approach is to use the diffusion model, here we use the DDPM model from the JoliGEN library. </p> <ul> <li>  Denoising Diffusion Probabilistic Models (DDPMs)   represent a class of generative models that provide a novel approach to synthesizing high-quality images. Particularly in the realm of image-to-image generation, such as inpainting and super-resolution, DDPMs exhibit excellent performance and results.  </li> <li>  In the arena of generative models, DDPMs have emerged as a strong contender to the likes of GANs and Variational Autoencoders (VAEs). The key principle behind DDPMs is the transformation of a simple noise distribution into a complex data distribution via a diffusion process that gradually adds or removes details over time.  </li> </ul> <p></p> <ul> <li>  The model essentially learns the data distribution of images, and through the gradual diffusion process, it can transform a noisy or incomplete image into a clean, detailed one.  </li> <li>  The significance of DDPMs in image-to-image tasks cannot be understated. The quality of the generated images is often superior to those from traditional GAN-based or VAE-based models. Furthermore, DDPMs have a more stable and easier-to-train architecture, which is another reason for their growing popularity.  </li> </ul> <p>  We need to create a dataset consisting of two directories <code>trainA</code> and <code>testA</code>. Each of these folder contains: </p> <ul> <li>  imgs:   the original images  </li> <li>  mask:   the masks for the hair area  </li> <li>  paths.txt:   the pairs image/mask used for training/testing  </li> </ul> <p> To get the mask of the hair we use the hair segmentation technique by the Mediapipe library. </p> <ul> <li> <p> For specific steps to create dataset, please see the tutorial here:   View colab tutorial  </p> </li> <li> <p> For specific steps to make this work with the DDPM model (JoliGEN), please see the source here:   View source  | \ud83d\udcf0 Paper </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#change-face","title":"Change face","text":"<p> During our experiments we also tried another approach that consist on changing the face of an individual in order to change its gender. Specifically, the augmentation we tried here were: -  Change the face of a blond hair female so that the new image is a blond hair male -  Change the face of a gray hair male so that the new image is a gray hair female</p>"},{"location":"api/experiments_summary/Bias_CelebA/#cut-cyclegan_1","title":"CUT + CycleGAN","text":"<p> Here we use the CUT and CycleGAN models from the JoliGEN library and the CycleGAN model that we programmed in Augmentare. We need to create a dataset consisting of two folders, one containing the images with the face we want to change (trainA) and another containing the facial images we are aiming for (trainB).  </p> <ul> <li> <p> For specific steps to make this work with CUT model (JoliGEN), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> <li> <p> For specific steps to make this work with the CycleGAN model (JoliGEN), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> <li> <p> For specific steps to make this work with the CycleGAN model (Augmentare), please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#diffusion-model_1","title":"Diffusion model","text":"<p> Another interesting approach is to use the diffusion model to insert an alternate face through a mask around the face, here we use the DDPM model from the JoliGEN library. We need to create a dataset consisting of two directories <code>trainA</code> and <code>trainB</code>. Each of these folder contains:     <ul> <li>  imgs:   the original images  </li> <li>  bbox:   contains a .txt file per image, that lists the boxes of face area of an individual and a mask .jpg file for the face area  </li> <li>  paths.txt:   list of associated source image/bbox file  </li> <p>  To get the bbox and face mask of the images, we use the Face Detection technique to create the bbox then use the bbox to create the mask for the face area.</p> <ul> <li>  Face Detection:   We tested the Face Detection methods such as: SSD, DFSDD, RNMV, RNRN50, MP then compared the accuracy of the face area and their calculation time. We choose the best method as `RNMV` to conduct our experiments.  </li> </ul> <p> Bounding boxes are elements location in format: <code>cls xmin ymin xmax ymax</code> where <code>cls</code> is the id of image. </p> <ul> <li> <p> For specific steps to create dataset, please see the tutorial here:   View colab tutorial  </p> </li> <li> <p> For specific steps to make this work with the DDPM model (JoliGEN), please see the source here:   View source  | \ud83d\udcf0 Paper </p> </li> </ul> <pre><code>fig = plt.figure(figsize=(9,8))\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/Diffusion/diffusion_face.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Diffusion Model\")\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We found that the DDPM model did not work in this case. Therefore we temporarily do not use it for our experiments. </p> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/DCGAN/male_blond.png\")\nax1.imshow(img)\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/DCGAN/female_gray.png\")\nax2.imshow(img)\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/ProGAN/male_blond.png\")\nax1.imshow(img)\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in CelebA/Outputs_DA/ProGAN/female_gray.png\")\nax2.imshow(img)\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre>"},{"location":"api/experiments_summary/Bias_CelebA/#other-gan-models","title":"Other GAN models","text":""},{"location":"api/experiments_summary/Bias_CelebA/#dcgan-progan","title":"DCGAN + ProGAN","text":"<p> We also tried DCGAN and ProGAN but we felt that the quality of the images generated was insufficient, moreover, it would have been necessary to generate a lot of images, then select them by hand according to the group of the image to be created. This seems to add an \"overlay\" quality that pushes us to set aside these approaches as part of our experiments. </p> <ul> <li> <p> For specific steps to make this work with the DCGAN model, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> <li> <p> We continue to try using another ProGAN model included in Augmentare to generate new images. They work fine and generate the images randomly. However, the image quality produced is not too good, so we did not choose the DCGAN and ProGAN models to perform our experiments. </p> </li> <li> <p> For specific steps to make this work with the ProGAN model, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#results-of-the-experiments","title":"Results of the experiments","text":"<p> After identifying useful Data Augmentation methods, we proceed to create new fake images and then add them to the original dataset and re-train the model and evaluate the results. After obtaining new minority images from the Data Augmentation methods, we add them to the original dataset until the number of minority classes accounts for 5%, 10%, 15%... with its parent image class. For example we add so that the number of blond hair male images accounts for 5%, 10%, 15% of the total number of male images, same for female. For each Data Augmentation method we perform the redistribution of the dataset with three experiments:     <ul> <li>  Only male:   redistributing only male images i.e. only adding blond hair male images to the original dataset  </li> <li>  Only female:   redistributing only female images i.e. only adding gray hair female images to the original dataset  </li> <li>  Both:   redistributing both male and female images i.e. only adding both blond hair male and gray hair female images images to the original dataset  </li> <p> </p> <ul> <li> <p> For specific steps to generate data by Classic method (OpenCV) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment  </p> </li> <li> <p> For specific steps to generate data by CUT_face (JoliGEN) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment  </p> </li> <li> <p> For specific steps to generate data by CUT_hair_color (JoliGEN) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment  </p> </li> <li> <p> For specific steps to generate data by CycleGAN (Augmentare) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment  </p> </li> <li> <p> For specific steps to generate data by CycleGAN_face (JoliGEN) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment  </p> </li> <li> <p> For specific steps to generate data by CycleGAN_face (JoliGEN) and train ResNet18 model, please see the tutorial here:   View colab tutorial  |   View source of experiment   Below we will plot the error rates of all image groups with Data Augmentation methods in the same figure for easy comparison. The horizontal dashed lines are the baseline of the image groups. </p> </li> </ul> <pre><code>df_female = pd.read_csv('dataframe_metrics/resnet18_female.csv')\n</code></pre> <pre><code>fig = plot_res_celeba_global(df=df_female, mode_redis=\"gray female\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = plot_best_res_celeba(df=df_female, mode_redis=\"gray female\", best_method=\"CyGAN_face/5\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = plot_best_res_celeba(df=df_female, mode_redis=\"gray female\", best_method=\"Over/15\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <ul> <li> <p> To determine which Data Augmentation method gives the best results, we will determine which method makes the error rates of different hair colors the most balanced for both male and female. </p> </li> <li> <p> In the case of redistribution only gray female image, we find that the CycleGAN_face/5 method achieves the best balance in terms of error rates between hair colors for male, however for female, it does not achieve the balance desired. However, in order to achieve a balance in error rates between male hair color groups, they must have a trade-off, namely, the black, red, and brown hair groups have a higher error rate than the baseline to find the balance. </p> </li> <li> <p> Therefore, based on good criteria for both male and female, the method with the best results is Oversampling/15. We see that most of the columns are below the baseline. However, there is still a trade-off in the gray hair male group. But this trade-off for more balance between groups of male's hair. </p> </li> </ul> <pre><code>df_male = pd.read_csv('dataframe_metrics/resnet18_male.csv')\n</code></pre> <pre><code>fig = plot_res_celeba_global(df=df_male, mode_redis=\"blond male\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = plot_best_res_celeba(df=df_male, mode_redis=\"blond male\", best_method=\"CyGAN_hair/10\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <ul> <li> Same as above, in the case of redistributing only blond male images, the method that achieves the best results is CycleGAN_hair_color/10. We see that for male we achieve a good balance. In the female class, however, we see that there is a trade-off, namely that the brown, black and blond hair color classes have a higher error rate than the baseline. But this trade-off for more balance between groups of female's hair.</li> </ul> <pre><code>df_total = pd.read_csv('dataframe_metrics/resnet18_total.csv')\n</code></pre> <pre><code>fig = plot_res_celeba_global(df=df_total, mode_redis=\"both gray female and blond male\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = plot_best_res_celeba(df=df_total, mode_redis=\"both gray female and blond male\", best_method=\"CUT_hair/15\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = plot_best_res_celeba(df=df_total, mode_redis=\"both gray female and blond male\", best_method=\"Over/15\", html=True, save_path = None)\nHTML(fig.to_html())\n</code></pre> <p> </p> <ul> <li> <p> For the case of redistributing both male and female images at the same time, the two methods that achieve the best results are Oversampling/15 and CUT_hair/15. The two methods do not differ much in the balance between classes, method with better balance is CUT_hair/15. Furthermore, in terms of trade-offs, the Oversampling/15 method has more classes with higher error rates than the baseline than the CUT_hair/15 method. So in this case, CUT_hair/15 is the best method. </p> </li> <li> <p> From the above cases, it is clear that in the case of redistributing only the number of male or female images, the results are sometimes even better than in the case of redistributing both at the same time. Therefore we should combine the redistribution of both male and female images with different proportions to get better results. </p> </li> </ul> <p> In this notebook we explored how to identify the minority groups present in the CelebA dataset and see how they bias AI models.   We have also implemented data augmentation methods for the minority groups that are blond hair male and gray hair female images. We perform a lot of different approaches to getting more minority images. Methods include Oversampling, Undersampling, Change Hair Color and Change Face.   We also set up an experiment to see which DA method worked best for correcting minorities. We run an experiments to see what percentage of minority images should be added to the original data for the best results. We have a few points to keep in mind: </p> <ul> <li> <p> It is worth noting here that Baseline is quite strong, it reaches Acc about 98%. We can aim to do something more complex than binary classification. </p> </li> <li> <p> In case of redistributing only gray female images, the best method is Oversampling/15. </p> </li> <li> <p> In case of redistributing only blond male images, the best method is CycleGAN_hair_color/10. </p> </li> <li> <p> In the case of redistributing both gray female and blond male images, the best method is CUT_hair_color/15.  </p> </li> <li> <p> Therefore, in general, the DA methods do not improve the bias much more than the Oversampling method. However, we can combine DA methods on male and female images with different enhancement ratios, which can achieve better results. Because we can see that, in the case of redistributing only male or female images, the result is sometimes better than distributing both at the same time. </p> </li> <li> <p> We can also study other strong biases that affect balance such as age and skin tone. </p> </li> </ul> <p> Limitations: </p> <ul> <li> <p> This task has not been tested much on different models. This is done only with pre-trained models. Moreover this experiment is only done with one baseline. Should try more baselines in the future. </p> </li> <li> <p> Have not trained the models many times with different seed values to see if the error rates of the groups are stable. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_CelebA/#only-redistribute-gray-female-images","title":"Only redistribute gray female images","text":""},{"location":"api/experiments_summary/Bias_CelebA/#only-redistribute-blond-male-images","title":"Only redistribute blond male images","text":""},{"location":"api/experiments_summary/Bias_CelebA/#redistribution-both-gray-female-and-blond-male-images","title":"Redistribution both gray female and blond male images","text":""},{"location":"api/experiments_summary/Bias_CelebA/#conclusions","title":"Conclusions","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/","title":"Summary","text":"<pre><code>import os\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image as PilImage\nfrom IPython.display import HTML\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom matplotlib import pyplot as plt\n\nfrom dataset import load_dataset_filtered_eurosat\n\nfrom utils import plot_wrong_pred, plot_bar_err_rate, plot_loss_acc_train,\\\n                plot_bar_acc_simu_simple, plot_err_rate_simu_simple,\\\n                plot_bar_acc_simu_double, plot_err_rate_simu_double,\\\n                plot_err_rate_test, plot_acc_test, plot_err_rate_test_double,\\\n                plot_heatmap_accuracy, plot_heatmap_err_rate, average_seeds\n</code></pre> <pre><code>ds_path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/data/EuroSAT\"\n</code></pre> <pre><code># (Re)set the seeds\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\n# Torch RNG\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n# Python RNG\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Load the dataset\nX_train, S_train, y_train, X_test, S_test, y_test = load_dataset_filtered_eurosat(path=ds_path)\nprint('EuroSAT dataset loaded !')\nprint('Training dataset (X,S,Y) : {} {} {}'.format(X_train.shape, S_train.shape, y_train.shape))\nprint('Testing dataset  (X,S,Y) : {} {} {}'.format(X_test.shape, S_test.shape, y_test.shape))\n</code></pre> <pre>\n<code>EuroSAT dataset loaded !\nTraining dataset (X,S,Y) : (3750, 3, 64, 64) (3750,) (3750,)\nTesting dataset  (X,S,Y) : (1250, 3, 64, 64) (1250,) (1250,)\n</code>\n</pre> <pre><code>highway_images = X_train[np.where(y_train==0)]\nhighway_labels = y_train[np.where(y_train==0)]\n\nblue_highway_images = X_train[np.where((S_train==0) &amp;amp; (y_train==0))]\nblue_highway_labels = y_train[np.where((S_train==0) &amp;amp; (y_train==0))]\n\nriver_images = X_train[np.where(y_train==1)]\nriver_labels = y_train[np.where(y_train==1)]\n\nblue_river_images = X_train[np.where((S_train==0) &amp;amp; (y_train==1))]\nblue_river_labels = y_train[np.where((S_train==0) &amp;amp; (y_train==1))]\n</code></pre> <pre><code>fig = go.Figure()\nx = ['Highway', 'River']\n\nfig.add_trace(go.Bar(\n    name='Blue',\n    x=x, y=[len(blue_highway_images) , len(blue_river_images)]))\n\nfig.add_trace(go.Bar(\n    name='Normal',\n    x=x, y=[len(highway_images) , len(river_images)]))\n\nfig.update_layout(title=\"Number of highway and river images on blue-veiled and normal property in train dataset\", yaxis_title=\"Number\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> We can see a big difference in the number of blue-veiled and normal properties in both highway and river images in the dataset. For example, the normal highway has 1875 images which is very large compared to the blue-veiled highway with 30 images, which is 98.4% less. The normal river with 1875 images is quite large compared to the blue-veiled river with 80 images, which is 95.74% less. This shows that the images with blue-veiled property account for a very small percentage of the EuroSAT dataset. </p> <pre><code>save_path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/Exp/Output_ResNet18\"\n</code></pre> <pre><code>acc, err_rate = average_seeds(path=f\"{save_path}/Baseline\", nb_seeds=10)\n</code></pre> <pre><code>fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Combination\", \"Property\", \"Images\"))\nx_img = ['Highway', 'River']\n\nfig.add_trace(go.Bar(\n    name='Blue-veiled',\n    x=x_img, y=[err_rate[0][0] , err_rate[0][1]]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Normal',\n    x=x_img, y=[err_rate[0][4] , err_rate[0][5]]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name=\"Property\",\n    x=x, y=err_rate[0][2:4]), row=1, col=2)\n\nfig.add_trace(go.Bar(\n    name=\"Images\",\n    x=x_img, y=err_rate[0][6:]), row=1, col=3)\n\nfig.update_layout(title=\"Error rate of image groups of Baseline (ResNet18)\", yaxis_title=\"Error rate\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> We find that minority image groups such as blue-veiled property have a significantly higher error rate than their normal property counterparts. This is exactly as we would expect, minority groups suffer from a significant error rate bias. </p> <pre><code>save_path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/Exp/Output_Vgg16\"\n</code></pre> <pre><code>acc, err_rate = average_seeds(path=f\"{save_path}/Baseline\", nb_seeds=10)\n</code></pre> <pre><code>fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Combination\", \"Property\", \"Images\"))\nx_img = ['Highway', 'River']\n\nfig.add_trace(go.Bar(\n    name='Blue-veiled',\n    x=x_img, y=[err_rate[0][0] , err_rate[0][1]]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Normal',\n    x=x_img, y=[err_rate[0][4] , err_rate[0][5]]), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name=\"Property\",\n    x=x, y=err_rate[0][2:4]), row=1, col=2)\n\nfig.add_trace(go.Bar(\n    name=\"Images\",\n    x=x_img, y=err_rate[0][6:]), row=1, col=3)\n\nfig.update_layout(title=\"Error rate of image groups of Baseline (ResNet18)\", yaxis_title=\"Error rate\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> In this case, the error rate of minority groups is smaller than that of the normal groups. Therefore, the bias of minority groups did not occur, this is because the Vgg16 model works too hard for the EuroSAT dataset, resulting in a low error rate. However we still Vgg16 for our experiments to see if the Data Augmentation methods improve the model's performance. </p> <p> The approach we immediately think of is to change the image style of the normal images to get the new blue-veiled images. An interesting method is Neural Style Transfer (NST), we will test methods including <code>FDA, StyleFlow, and CCPL</code> to change the style of normal images.   To make it easier to understand, we will actually take 2 images, one with the content we want to style, and another with the style we are aiming for. Then use the methods we introduced above to extract the style from the second image then combine it with the content of the first image to create a new image with the content and style we desire. To better visualize the <code>NST</code> methods, please see the image below. On the left is the image with the content we want to change the style, the small image in the bottom corner is the styled image we are aiming for (the image is blue-veiled), and the image on the right is the image that has been created after combining content and style.  </p> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/FDA/hw_fda.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by FDA\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/FDA/rv_fda.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by FDA\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/Flow/hw_flow.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by StyleFlow\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/Flow/rv_flow.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by StyleFlow\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/CCPL/hw_ccpl.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by CCPL\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/CCPL/rv_ccpl.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by CCPL\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/hw_blue.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by DCGAN\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/rv_blue.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by DCGAN\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <p> We can see that the DCGAN model works well and produces not-bad normal and blue images. However, the generated blue images are relatively different from the minority blue-veiled images, especially the poorly generated river blue images. Compared with the AutoEncoder model, DCGAN gives better quality results, but the generated samples are quite similar with no diversity, namely normal highway images. </p> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/DCGAN_FDA/hw_dcgan_fda.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by FDA\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/DCGAN_FDA/rv_dcgan_fda.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by FDA\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/hw_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal Highway by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/DCGAN_Flow/hw_dcgan_flow.png\")\nax2.imshow(img)\nax2.set_title(\"Blue Highway by Flow\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,5))\n\nax1 = fig.add_subplot(1, 2, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/GAN_output/rv_normal.png\")\nax1.imshow(img)\nax1.set_title(\"Normal River by DCGAN\")\nax1.axis(\"off\")\n\nax2 = fig.add_subplot(1, 2, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/DCGAN_Flow/rv_dcgan_flow.png\")\nax2.imshow(img)\nax2.set_title(\"Blue River by Flow\")\nax2.axis(\"off\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>nbs_highway = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\nnbs_river = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\n</code></pre> <pre><code>path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/NST_output/FDA/ResNet18\"\naccuracy, error_rate = average_seeds(path=path, nb_seeds=10)\n</code></pre> <pre><code>plot_heatmap_accuracy(accuracy, nbs_highway, nbs_river)\n</code></pre> <p> The figure above is the result of the <code>Accuracy</code> assessment after adding new blue images. We see that the pair of blue images highway and river giving the best Acc of <code>98.46%</code> are <code>[600,200]</code> and <code>[0,500]</code>. This highest ACC does not improve much compared to the Baseline <code>98.35%</code> (corresponding to the pair of numbers <code>[0,0]</code>). We can see on the heat chart figure there is a clear difference but basically, the value is not much different, we can see that the right heat column shows the lowest value of <code>97.8</code> and the highest value is <code>98.4</code>, there is no excessive difference between the min and max values. </p> <pre><code>plot_heatmap_err_rate(error_rate, nbs_highway, nbs_river)\n</code></pre> <p> Next, we will plot heat maps of error rates across different groups. We will choose the most optimal pair of blue images highway and river of each Data Augmentation method. That is, it makes the error ratio between blue images and normal images the most balanced. </p> <pre><code>df_resnet18 = pd.read_csv('/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/Exp/new_resnet18_dataframe.csv')\n</code></pre> <pre><code>df_vgg16 = pd.read_csv('/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/Exp/new_vgg16_dataframe.csv')\n</code></pre> <pre><code>frames = [df_resnet18, df_vgg16]\nresult = pd.concat(frames)\n</code></pre> <pre><code>array = [\"Baseline\", \"Aug\"]\ndataframe = result[result['Method'].isin(array)]\n</code></pre> <pre><code>bl = dataframe[dataframe['Metric name']==\"Err_bl\"]\nnor = dataframe[dataframe['Metric name']==\"Err_nor\"]\n</code></pre> <pre><code>fig = px.box(bl, y='Metric value', color='Augmentation method', x='Method', facet_col='Model name', height=500, width=1200,\ntitle='Comparison the error rate of blue images with different data augmentation methods',\ncolor_discrete_sequence=[\"red\", \"green\", \"blue\", \"purple\", \"cyan\", \"darkorange\", \"darkseagreen\", \"deeppink\", \"firebrick\", \"yellowgreen\", \"indianred\", \"teal\", \"olive\"])\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> We can see that the <code>STyleFlow</code> data augmentation method has worked well in reducing the error rate of blue images both <code>ResNet18</code> and <code>Vgg16</code> models.     <ul> <li>  First:   For the ResNet18 model, the Data Augmentation methods that achieve the best results are Flow and Oversampling/15. As for the Vgg16 model, the best DA method is Flow.  </li> <li>  Second:   Another point to note is that after combining `DCGAN` and `FDA`, the results seem to be better than when we only use `FDA`.  </li> <p>  We can easily see that on the box plot corresponding to 10 different seeds, the error rate value is not stable, the variation is quite large. For ResNet18 there are 2 methods Flow and Oversampling/15 are relatively stable.  </p> <pre><code>fig = px.box(nor, y='Metric value', color='Augmentation method', x='Method', facet_col=\"Model name\", height=500, width=1200,\ntitle='Comparison the error rate of normal images with different data augmentation methods',\ncolor_discrete_sequence=[\"red\", \"green\", \"blue\", \"purple\", \"cyan\", \"darkorange\", \"darkseagreen\", \"deeppink\", \"firebrick\", \"yellowgreen\", \"indianred\", \"teal\", \"olive\"])\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> There is a contradiction here that for the <code>ResNet18</code> model after data augmentation the error rate of normal images usually seems to be increased, and for the <code>Vgg16</code> model the error rate of normal images is usually reduced along with the blue images. Like the blue images, the normal images also have instability errors corresponding to 10 seed values. </p> <pre><code>def get_mean_std(dataframe):\n    data = {}\n    data[\"Augmentation method\"] = dataframe[\"Augmentation method\"].unique()\n\n    for model in (dataframe[\"Model name\"].unique()):\n        for met in (dataframe[\"Method\"].unique()):\n            list_std = []\n            list_mean = []\n            for aug in (dataframe[\"Augmentation method\"].unique()):\n                std = dataframe.loc[(dataframe[\"Model name\"] == model) &amp;amp; (dataframe[\"Augmentation method\"] == aug) &amp;amp; (dataframe[\"Method\"] == met), [\"Metric value\"]].std(ddof=0).item()\n                m = dataframe.loc[(dataframe[\"Model name\"] == model) &amp;amp; (dataframe[\"Augmentation method\"] == aug) &amp;amp; (dataframe[\"Method\"] == met), [\"Metric value\"]].mean().item()\n                list_std.append(std)\n                list_mean.append(m)\n\n            data[model, met] = list_mean\n            data[model, f\"{met}_std\"] = list_std\n\n    df = pd.DataFrame(data).set_index(\"Augmentation method\")\n    df.columns = pd.MultiIndex.from_tuples(df.columns)\n    return df\n</code></pre> <pre><code>df_bl = get_mean_std(bl)\n</code></pre> <pre><code>df_bl\n</code></pre> ResNet18 Vgg16 Baseline Baseline_std Aug Aug_std Baseline Baseline_std Aug Aug_std Augmentation method FDA 2.777778 1.707323 1.481481 1.385799 2.037037 2.103299 1.666667 1.747034 Flow 2.777778 1.707323 2.037037 0.997253 2.037037 2.103299 0.925926 0.925926 CCPL 2.777778 1.707323 3.888889 2.103299 2.037037 2.103299 2.777778 3.014596 DCGAN_FDA 2.777778 1.707323 1.481481 1.111111 2.037037 2.103299 1.481481 1.385799 DCGAN_Flow 2.777778 1.707323 1.666667 1.933390 2.037037 2.103299 2.592593 2.371527 Oversampling/5 2.777778 1.707323 2.592593 1.228380 2.037037 2.103299 2.037037 1.538264 Oversampling/10 2.777778 1.707323 1.296296 1.185764 2.037037 2.103299 2.407407 1.185764 Oversampling/15 2.777778 1.707323 1.851852 1.171214 2.037037 2.103299 1.666667 1.933390 Oversampling/20 2.777778 1.707323 1.666667 1.538264 2.037037 2.103299 1.481481 1.614407 Undersampling/5 2.777778 1.707323 4.074074 1.614407 2.037037 2.103299 5.740741 2.407407 Undersampling/10 2.777778 1.707323 4.814815 2.222222 2.037037 2.103299 6.111111 3.317865 Undersampling/15 2.777778 1.707323 8.518519 3.120796 2.037037 2.103299 9.629630 2.721655 Undersampling/20 2.777778 1.707323 7.592593 2.103299 2.037037 2.103299 9.814815 2.349737 <pre><code>fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ResNet18\", \"Vgg16\"))\nx = ['FDA', 'Flow', 'CCPL', 'DCGAN_FDA', 'DCGAN_Flow', 'Oversampling/5', 'Oversampling/10', 'Oversampling/15', 'Oversampling/20', 'Undersampling/5', 'Undersampling/10', 'Undersampling/15', 'Undersampling/20']\n\nfig.add_trace(go.Bar(\n    name='Baseline ResNet18',\n    x=x, y=df_bl.ResNet18.Baseline,\n    error_y=dict(type='data', array=df_bl.ResNet18.Baseline_std)\n), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Augmentation ResNet18',\n    x=x, y=df_bl.ResNet18.Aug,\n    error_y=dict(type='data', array=df_bl.ResNet18.Aug_std)\n), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Baseline Vgg16',\n    x=x, y=df_bl.Vgg16.Baseline,\n    error_y=dict(type='data', array=df_bl.Vgg16.Baseline_std)\n), row=1, col=2)\n\nfig.add_trace(go.Bar(\n    name='Augmentation Vgg16',\n    x=x, y=df_bl.Vgg16.Aug,\n    error_y=dict(type='data', array=df_bl.Vgg16.Aug_std)\n), row=1, col=2)\n\nfig.update_xaxes(title_text=\"Data Augmentation Methods\", row=1, col=1)\nfig.update_xaxes(title_text=\"Data Augmentation Methods\", row=1, col=2)\nfig.update_layout(title=\"Average error rate of blue images according to different data augmentation methods\", yaxis_title=\"Error rate\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <ul> <li> <p> <code>ResNet18:</code> We see that both the <code>FDA</code> and <code>DCGAN_FDA</code> methods achieve an error rate of as little as 1.48 but the <code>DCGAN_FDA</code> method obtains a smaller std than the <code>FDA</code> method. <code>DCGAN_Flow</code> method achieves better results than <code>Flow</code> but std is higher. The method to achieve the least error is Oversampling/10. </p> </li> <li> <p> <code>Vgg 16:</code> The <code>Flow</code> method with the best results followed by the <code>DCGAN_FDA</code> method and then the <code>FDA</code>. </p> </li> </ul> <pre><code>df_nor = get_mean_std(nor)\n</code></pre> <pre><code>df_nor\n</code></pre> ResNet18 Vgg16 Baseline Baseline_std Aug Aug_std Baseline Baseline_std Aug Aug_std Augmentation method FDA 1.195652 0.204977 1.780936 0.330347 3.854515 0.463199 3.586957 0.464705 Flow 1.195652 0.204977 1.647157 0.317396 3.854515 0.463199 3.469900 0.465907 CCPL 1.195652 0.204977 2.224080 0.267559 3.854515 0.463199 4.105351 0.343624 DCGAN_FDA 1.195652 0.204977 1.530100 0.274904 3.854515 0.463199 3.369565 0.394043 DCGAN_Flow 1.195652 0.204977 1.680602 0.228828 3.854515 0.463199 3.687291 0.455590 Oversampling/5 1.195652 0.204977 1.672241 0.250836 3.854515 0.463199 3.620401 0.474531 Oversampling/10 1.195652 0.204977 1.931438 0.433575 3.854515 0.463199 3.628763 0.514333 Oversampling/15 1.195652 0.204977 1.638796 0.198568 3.854515 0.463199 3.603679 0.532038 Oversampling/20 1.195652 0.204977 1.479933 0.272349 3.854515 0.463199 3.461538 0.345147 Undersampling/5 1.195652 0.204977 3.779264 0.894922 3.854515 0.463199 6.279264 0.830626 Undersampling/10 1.195652 0.204977 5.200669 1.255184 3.854515 0.463199 8.177258 0.725837 Undersampling/15 1.195652 0.204977 7.675585 1.538916 3.854515 0.463199 8.954849 1.109523 Undersampling/20 1.195652 0.204977 8.578595 2.125194 3.854515 0.463199 10.234114 1.177239 <pre><code>fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ResNet18\", \"Vgg16\"))\nx = ['FDA', 'Flow', 'CCPL', 'DCGAN_FDA', 'DCGAN_Flow', 'Oversampling/5', 'Oversampling/10', 'Oversampling/15', 'Oversampling/20', 'Undersampling/5', 'Undersampling/10', 'Undersampling/15', 'Undersampling/20']\n\nfig.add_trace(go.Bar(\n    name='Baseline ResNet18',\n    x=x, y=df_nor.ResNet18.Baseline,\n    error_y=dict(type='data', array=df_nor.ResNet18.Baseline_std)\n), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Augmentation ResNet18',\n    x=x, y=df_nor.ResNet18.Aug,\n    error_y=dict(type='data', array=df_nor.ResNet18.Aug_std)\n), row=1, col=1)\n\nfig.add_trace(go.Bar(\n    name='Baseline Vgg16',\n    x=x, y=df_nor.Vgg16.Baseline,\n    error_y=dict(type='data', array=df_nor.Vgg16.Baseline_std)\n), row=1, col=2)\n\nfig.add_trace(go.Bar(\n    name='Augmentation Vgg16',\n    x=x, y=df_nor.Vgg16.Aug,\n    error_y=dict(type='data', array=df_nor.Vgg16.Aug_std)\n), row=1, col=2)\n\nfig.update_xaxes(title_text=\"Data Augmentation Methods\", row=1, col=1)\nfig.update_xaxes(title_text=\"Data Augmentation Methods\", row=1, col=2)\nfig.update_layout(title=\"Average error rate of normal images according to different data augmentation methods\", yaxis_title=\"Error rate\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <ul> <li> We again see that the error rate of normal images with the <code>Vgg16</code> model is essentially reduced but increases for the <code>ResNet18</code> model. </li> </ul> <pre><code>fig = plt.figure(figsize=(9,6))\n\nax1 = fig.add_subplot(1, 4, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/hw_ori.png\")\nax1.imshow(img)\nax1.axis(\"off\")\nax1.set_title(\"Baseline (Highway)\")\n\nax2 = fig.add_subplot(1, 4, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/hw_flow.png\")\nax2.imshow(img)\nax2.axis(\"off\")\nax2.set_title(\"Flow (Highway)\")\n\nax3 = fig.add_subplot(1, 4, 3)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/rv_ori.png\")\nax3.imshow(img)\nax3.axis(\"off\")\nax3.set_title(\"Baseline (River)\")\n\nax4 = fig.add_subplot(1, 4, 4)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/rv_flow.png\")\nax4.imshow(img)\nax4.axis(\"off\")\nax4.set_title(\"Flow (River)\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>fig = plt.figure(figsize=(9,6))\n\nax1 = fig.add_subplot(1, 4, 1)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/Vgg16/hw_ori.png\")\nax1.imshow(img)\nax1.axis(\"off\")\nax1.set_title(\"Baseline (Highway)\")\n\nax2 = fig.add_subplot(1, 4, 2)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/Vgg16/hw_flow.png\")\nax2.imshow(img)\nax2.axis(\"off\")\nax2.set_title(\"Flow (Highway)\")\n\nax3 = fig.add_subplot(1, 4, 3)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/Vgg16/rv_ori.png\")\nax3.imshow(img)\nax3.axis(\"off\")\nax3.set_title(\"Baseline (River)\")\n\nax4 = fig.add_subplot(1, 4, 4)\nimg = PilImage.open(f\"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/Vgg16/rv_flow.png\")\nax4.imshow(img)\nax4.axis(\"off\")\nax4.set_title(\"Flow (River)\")\n\nfig.tight_layout()\nfig.show()\n</code></pre> <pre><code>df = pd.read_csv('/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/xai_dataframe.csv')\n</code></pre> <pre><code>del_vgg = df[(df['Metric name']==\"Deletion\") &amp;amp; (df['Model name']==\"Vgg16\")]\nins_vgg = df[(df['Metric name']==\"Insertion\") &amp;amp; (df['Model name']==\"Vgg16\")]\n</code></pre> <pre><code>del_vgg_hw = del_vgg[del_vgg['Image class']==\"Highway\"]\ndel_vgg_rv = del_vgg[del_vgg['Image class']==\"River\"]\n</code></pre> <pre><code>ins_vgg_hw = ins_vgg[ins_vgg['Image class']==\"Highway\"]\nins_vgg_rv = ins_vgg[ins_vgg['Image class']==\"River\"]\n</code></pre> <pre><code>fig = px.box(del_vgg_hw, y='Metric value', x='XAI method', color='Model type', facet_col=\"Prediction\", height=600, width=1200,\ntitle='Comparison the Deletion score of Highway images (Vgg16)')\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>fig = px.box(ins_vgg_hw, y='Metric value', x='XAI method', color='Model type', facet_col=\"Prediction\", height=600, width=1200,\ntitle='Comparison the Insertion score of Highway images (Vgg16)')\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> The XAI methods explained the difference between the true and false predictions of the model. In fact, in the case of wrong predictions, the XAI methods also do not work well. </p> <pre><code>def get_mean_std(dataframe):\n    data = {}\n    data[\"XAI method\"] = dataframe[\"XAI method\"].unique()\n\n    for model in (dataframe[\"Model name\"].unique()):\n        for met in (dataframe[\"Model type\"].unique()):\n            list_std = []\n            list_mean = []\n            for xai in (dataframe[\"XAI method\"].unique()):\n                std = dataframe.loc[(dataframe[\"Model name\"] == model) &amp;amp; (dataframe[\"XAI method\"] == xai) &amp;amp; (dataframe[\"Model type\"] == met), [\"Metric value\"]].std(ddof=0).item()\n                m = dataframe.loc[(dataframe[\"Model name\"] == model) &amp;amp; (dataframe[\"XAI method\"] == xai) &amp;amp; (dataframe[\"Model type\"] == met), [\"Metric value\"]].mean().item()\n                list_std.append(std)\n                list_mean.append(m)\n\n            data[model, met] = list_mean\n            data[model, f\"{met}_std\"] = list_std\n\n    df = pd.DataFrame(data).set_index(\"XAI method\")\n    df.columns = pd.MultiIndex.from_tuples(df.columns)\n    return df\n</code></pre> <pre><code>df_hw = df[df['Image class']==\"Highway\"]\ndf_rv = df[df['Image class']==\"River\"]\n</code></pre> <pre><code>del_hw = df_hw[(df_hw['Metric name']==\"Deletion\") &amp;amp; (df_hw['Prediction']==True)]\nins_hw = df_hw[(df_hw['Metric name']==\"Insertion\") &amp;amp; (df_hw['Prediction']==True)]\n\ndel_rv = df_rv[(df_rv['Metric name']==\"Deletion\") &amp;amp; (df_rv['Prediction']==True)]\nins_rv = df_rv[(df_rv['Metric name']==\"Insertion\") &amp;amp; (df_rv['Prediction']==True)]\n</code></pre> <pre><code>df_del_hw = get_mean_std(del_hw)\ndf_del_rv = get_mean_std(del_rv)\n</code></pre> <pre><code>df_vgg = df_del_hw.Vgg16\n</code></pre> <pre><code>fig = go.Figure()\nx = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\n\nfig.add_trace(go.Bar(\n    name='Baseline',\n    x=x, y=df_vgg.Baseline,\n    error_y=dict(type='data', array=df_vgg.Baseline_std)\n))\nfig.add_trace(go.Bar(\n    name='FDA',\n    x=x, y=df_vgg.FDA,\n    error_y=dict(type='data', array=df_vgg.FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='Flow',\n    x=x, y=df_vgg.Flow,\n    error_y=dict(type='data', array=df_vgg.Flow_std)\n))\nfig.add_trace(go.Bar(\n    name='CCPl',\n    x=x, y=df_vgg.CCPL,\n    error_y=dict(type='data', array=df_vgg.CCPL_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_FDA',\n    x=x, y=df_vgg.DCGAN_FDA,\n    error_y=dict(type='data', array=df_vgg.DCGAN_FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_Flow',\n    x=x, y=df_vgg.DCGAN_Flow,\n    error_y=dict(type='data', array=df_vgg.DCGAN_Flow_std)\n))\n\nfig.update_layout(title=\"Deletion metric of Highway images of Vgg16\", xaxis_title=\"XAI method\", yaxis_title=\"Deletion\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>df_vgg = df_del_rv.Vgg16\n</code></pre> <pre><code>fig = go.Figure()\nx = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\n\nfig.add_trace(go.Bar(\n    name='Baseline',\n    x=x, y=df_vgg.Baseline,\n    error_y=dict(type='data', array=df_vgg.Baseline_std)\n))\nfig.add_trace(go.Bar(\n    name='FDA',\n    x=x, y=df_vgg.FDA,\n    error_y=dict(type='data', array=df_vgg.FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='Flow',\n    x=x, y=df_vgg.Flow,\n    error_y=dict(type='data', array=df_vgg.Flow_std)\n))\nfig.add_trace(go.Bar(\n    name='CCPl',\n    x=x, y=df_vgg.CCPL,\n    error_y=dict(type='data', array=df_vgg.CCPL_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_FDA',\n    x=x, y=df_vgg.DCGAN_FDA,\n    error_y=dict(type='data', array=df_vgg.DCGAN_FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_Flow',\n    x=x, y=df_vgg.DCGAN_Flow,\n    error_y=dict(type='data', array=df_vgg.DCGAN_Flow_std)\n))\n\nfig.update_layout(title=\"Deletion metric of River images of Vgg16\", xaxis_title=\"XAI method\", yaxis_title=\"Deletion\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>df_ins_hw = get_mean_std(ins_hw)\ndf_ins_rv = get_mean_std(ins_rv)\n</code></pre> <pre><code>df_vgg = df_ins_hw.Vgg16\n</code></pre> <pre><code>fig = go.Figure()\nx = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\n\nfig.add_trace(go.Bar(\n    name='Baseline',\n    x=x, y=df_vgg.Baseline,\n    error_y=dict(type='data', array=df_vgg.Baseline_std)\n))\nfig.add_trace(go.Bar(\n    name='FDA',\n    x=x, y=df_vgg.FDA,\n    error_y=dict(type='data', array=df_vgg.FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='Flow',\n    x=x, y=df_vgg.Flow,\n    error_y=dict(type='data', array=df_vgg.Flow_std)\n))\nfig.add_trace(go.Bar(\n    name='CCPl',\n    x=x, y=df_vgg.CCPL,\n    error_y=dict(type='data', array=df_vgg.CCPL_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_FDA',\n    x=x, y=df_vgg.DCGAN_FDA,\n    error_y=dict(type='data', array=df_vgg.DCGAN_FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_Flow',\n    x=x, y=df_vgg.DCGAN_Flow,\n    error_y=dict(type='data', array=df_vgg.DCGAN_Flow_std)\n))\n\nfig.update_layout(title=\"Insertion metric of Highway images of Vgg16\", xaxis_title=\"XAI method\", yaxis_title=\"Insertion\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <pre><code>df_vgg = df_ins_rv.Vgg16\n</code></pre> <pre><code>fig = go.Figure()\nx = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\n\nfig.add_trace(go.Bar(\n    name='Baseline',\n    x=x, y=df_vgg.Baseline,\n    error_y=dict(type='data', array=df_vgg.Baseline_std)\n))\nfig.add_trace(go.Bar(\n    name='FDA',\n    x=x, y=df_vgg.FDA,\n    error_y=dict(type='data', array=df_vgg.FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='Flow',\n    x=x, y=df_vgg.Flow,\n    error_y=dict(type='data', array=df_vgg.Flow_std)\n))\nfig.add_trace(go.Bar(\n    name='CCPl',\n    x=x, y=df_vgg.CCPL,\n    error_y=dict(type='data', array=df_vgg.CCPL_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_FDA',\n    x=x, y=df_vgg.DCGAN_FDA,\n    error_y=dict(type='data', array=df_vgg.DCGAN_FDA_std)\n))\nfig.add_trace(go.Bar(\n    name='DCGAN_Flow',\n    x=x, y=df_vgg.DCGAN_Flow,\n    error_y=dict(type='data', array=df_vgg.DCGAN_Flow_std)\n))\n\nfig.update_layout(title=\"Insertion metric of River images of Vgg16\", xaxis_title=\"XAI method\", yaxis_title=\"Insertion\", barmode=\"group\")\n#fig.show()\nHTML(fig.to_html())\n</code></pre> <p> </p> <p> Based on the criteria of metrics, for Deletion lower is better and Insertion higher is better. We find that CAM methods explain Highway images well, while methods like Ig, Guidedbp, and Shap explain better with River images. </p> <pre><code>path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/bas\"\n\nfig = plt.figure(figsize=(9,10))\nrows = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\nmethods = ['grad_cam', 'grad_cam_plus', 'layer_cam', 'score_cam', 'ig', 'guidedbp', 'shap']\n\ngs = fig.add_gridspec(7,2)\nfor num in range(1,8):\n    ax = fig.add_subplot(gs[num-1,0])\n    img = PilImage.open(f\"{path}/del_bon_hw_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.set_ylabel(rows[num-1], rotation=0, fontsize=15, labelpad=50)\n    ax.yaxis.label.set_color('black')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[num-1,1])\n    img = PilImage.open(f\"{path}/ins_bon_hw_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.axis(\"off\")\n\nfig.tight_layout()\nplt.show()\n</code></pre> <p> Here we take an example of a highway image and see how the Deletion and Insertion scores play out. We clearly see that CAM methods have better results than others. Specifically for Deletion, the area below the blue line is quite small, and above all, the blue line decreases to zero very quickly. As for Insertion, the blue graph increases rapidly and gets close to 1, making the area below set a high score.   However, there is a point to note here that in the case of Deletion of CAM methods, there is a peak at the end of the stroke. This is because the classification model we use is not good enough. </p> <pre><code>path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18/bas\"\n\nfig = plt.figure(figsize=(9,10))\nrows = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\nmethods = ['grad_cam', 'grad_cam_plus', 'layer_cam', 'score_cam', 'ig', 'guidedbp', 'shap']\n\ngs = fig.add_gridspec(7,2)\nfor num in range(1,8):\n    ax = fig.add_subplot(gs[num-1,0])\n    img = PilImage.open(f\"{path}/del_bon_rv_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.set_ylabel(rows[num-1], rotation=0, fontsize=15, labelpad=50)\n    ax.yaxis.label.set_color('black')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[num-1,1])\n    img = PilImage.open(f\"{path}/ins_bon_rv_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.axis(\"off\")\n\nfig.tight_layout()\nplt.show()\n</code></pre> <p> Now let's look at an example of a river image to see if the result matches our comments in the previous section. We can clearly see that although Ig, Shap, and Guidedbp methods don't work very well, it is better than CAM methods both in terms of Deletion and Insertion game. </p> <pre><code>path = \"/home/vuong.nguyen/vuong/augmentare/experiments/Bias in EuroSAT/XAI_Output/ResNet18\"\n\nfig = plt.figure(figsize=(9,10))\nrows = ['GradCAM', 'GradCAMPlus', 'LayerCAM', 'ScoreCAM', 'Ig', 'Guidedbp', 'Shap']\nmethods = ['grad_cam', 'grad_cam_plus', 'layer_cam', 'score_cam', 'ig', 'guidedbp', 'shap']\n\ngs = fig.add_gridspec(7,3)\nfor num in range(1,8):\n    ax = fig.add_subplot(gs[num-1,0])\n    img = PilImage.open(f\"{path}/bas/del_bon_hw_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.set_ylabel(rows[num-1], rotation=0, fontsize=15, labelpad=50)\n    ax.yaxis.label.set_color('black')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if num == 1:\n        ax.set_title(\"Baseline\")\n\n    ax = fig.add_subplot(gs[num-1,1])\n    img = PilImage.open(f\"{path}/fda/del_bon_hw_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.axis(\"off\")\n    if num == 1:\n        ax.set_title(\"FDA\")\n\n    ax = fig.add_subplot(gs[num-1,2])\n    img = PilImage.open(f\"{path}/dcgan_fda/del_bon_hw_{methods[num-1]}.png\")\n    ax.imshow(img, aspect='auto')\n    ax.axis(\"off\")\n    if num == 1:\n        ax.set_title(\"DCGAN_FDA\")\n\nfig.tight_layout()\nplt.show()\n</code></pre> <p> Here we will compare the baseline model with the FDA and DCGAN_FDA. We see that the Deletion curve of the model trained with FDA-generated data no longer has peaks at the end. This proves that the model enhanced by the FDA method is better than the Baseline.   The model trained with data generated by DCGAN_FDA also performed well. We can see its Deletion line is sometimes even smoother than the FDA specifically in the GradCAMPlus interpretation method. Another point worth noting is that for Ig, Guidedbp, and Shap interpretation methods the FDA's Deletion line is not much better. However, it is clear that the Deletion line of DCGAN_FDA has improved significantly and is much smoother. This proves that the combination of DCGAN and FDA is better than using FDA alone. </p> <p> In this notebook we explored how to identify the minority groups present in the EuroSAT dataset and see how they bias AI models. Also note that it sometimes produces bias for one model but not for another.   We have also implemented data augmentation methods for the minority groups that are blue-veiled images. Data Augmentation methods include Oversampling, Undersampling, FDA, StyleFlow, CCPL, and a combination of DCGAN_FDA and DCGAN_Flow.   We also set up an experiment to see which DA method worked best for correcting minorities. We run an experiment to see how many images of the minority class should be added to the original data for the best results. To see if minority error rates were stable we also trained models with 10 different seed values. We have a few points to note as follows: </p> <ul> <li> <p> It is worth noting here that Baseline is quite strong, it reaches Acc about 97%. We can aim to do something more complex than binary classification. </p> </li> <li> <p> In general, the DA methods improve the error rate of the minority group of images that are blue images. However we can see that the CCPL method does not work in this case. </p> </li> <li> <p> After using DA methods the error rate of normal images with ResNet18 increased a bit. For the Vgg16 model, the error rate of the normal image is also improved. </p> </li> <li> <p> The combination of DCGAN_FDA or DCGAN_Flow works better than FDA or Flow on ResNet18 models but not on Vgg16.   We also performed XAI methods to explain the differences between the baseline model and the data-augmented model.</p> </li> <li> <p> We looked at the difference between the true and false predictions of the model. </p> </li> <li> <p> See which XAI method works well with which image classes in the dataset. </p> </li> <li> <p> See which DA method helps the model achieve the best results. </p> </li> </ul> <p> Limitations: </p> <ul> <li> <p> This task has not been tested much on different models. This is done only with pre-trained models. </p> </li> <li> <p> Not many combinations between GAN and NST models have been made. </p> </li> <li> <p> Haven't tested on other datasets. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#bias-in-eurosat","title":"Bias in EuroSAT","text":"<p> Author - Vuong NGUYEN </p> <ul> <li> <p> The EuroSAT dataset is a land-cover satellite image classification use-case on 13 spectral bands consisting on 10 classes and containing 27000 labeled and geo-referenced examples. As we will be placing ourselves in the fairness framework, we will be simplyfying our problem to a Highway/River binary classification problem, where we also know that a small percentage of images (~3\\% of the whole dataset) has a certain blue-veiled property. As we will prove later, this group of images constitutes a discriminated group in the sense of the equality of odds/errors. </p> </li> <li> <p> To identify these images, we will transform the images to the HSV color-space, and use custom thresholds in the mean of each channel to flag each image as either discriminated (blue-veiled, S=0) or not (S=1). In particular, we fix this threshold at 0.35 for the saturation channel, 0.4 for the value, and verify that the hue's value is between 210 and 270. With this information, we can calculate some statistics about the dataset. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#imports","title":"Imports","text":"<p> We import the necessary libraries and tools. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#exploring-eurosat","title":"Exploring EuroSAT","text":"<p> Let's take a look at the distribution of the number of highway and river images on blue-veiled and normal properties in this dataset. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#baseline-resnet18-vgg16","title":"Baseline (ResNet18 + Vgg16)","text":"<p> To evaluate if the minority groups highlighted previously creates a bias in the learning an AI model, we take a <code>ResNet18</code> and a <code>Vgg16</code> model as a baseline model for comparison of error rates between groups.   One caveat here is that we trained all the classification models 10 times corresponding to different seed values. The purpose here is to see if, with different seed values, the error rate of the classification models changes much. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#resnet18","title":"ResNet18","text":"<ul> <li> For specific steps to make this work with <code>ResNet18</code> model, please see the tutorial here:   View source  </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#vgg16","title":"Vgg16","text":"<ul> <li> For specific steps to make this work with <code>Vgg16</code> model, please see the tutorial here:   View source  </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#data-augmentation","title":"Data Augmentation","text":"<p> So the question is how to handle this bias. One way to solve this problem is to implement Data Augmentation methods by creating new images of the minority groups here is blue-veiled images, then adding the augmented images to the original dataset and retraining the model. In this work, we investigated several Data Augmentation methods to evaluate their ability to mitigate the bias. Next subsections introduce the different methods we used on the EuroSAT dataset.</p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#oversampling","title":"Oversampling","text":"<ul> <li> <p> Oversampling is a technique in machine learning used to address class imbalance. It involves increasing the number of instances in the minority groups by generating synthetic samples or replicating existing ones. This aims to balance the distribution of classes in the training dataset, which can improve the performance of machine learning models, particularly in binary classification tasks. </p> </li> <li> <p> In this case, we use the Oversampling method to replicate the images of minorities <code>(blue-veiled highway and blue-veiled river)</code> into the original dataset. We will replicate the images of the minority group into the original dataset until this minority group reaches the percentage of 5%, 10%,... compared to the total number of images of the class to be predicted <code>(highway and river)</code> in the dataset for our task. In summary, Oversampling increases the representativeness of the minority group like <code>blue-veiled highway and blue-veiled river</code>. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#undersampling","title":"Undersampling","text":"<ul> <li> <p> Undersampling is another technique used to mitigate class imbalance in machine learning datasets. It focuses on reducing the number of instances in the majority group to balance it with the minority group. This is typically done by randomly removing examples from the majority group. Undersampling can help prevent models from being biased toward the majority group and can improve their ability to recognize the minority group. </p> </li> <li> <p> In this case, we use the Undersampling method to randomly removing the images of the majority group <code>(normal highway and normal river)</code> from the original dataset until the minority group <code>(blue-veiled highway and blue-veiled river)</code> reach the ratio 5%, 10%,... compared to the total number of images of the class to be predicted <code>(highway and river)</code> in the dataset for our task. In summary, Oversampling reduces the representativeness of the majority group like <code>normal highway and normal river</code>. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#neural-style-transfer","title":"Neural Style Transfer","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#fourrier-domain-adaptation-fda","title":"Fourrier Domain Adaptation (FDA)","text":"<p> Simplified domain adaptation via style transfer thanks to the Fourier transformation. The FDA does not need deep networks for style transfer and adversarial training.   The scheme of the proposed Fourier domain adaptation method: </p> <ul> <li>  Step 1:   Apply FFT to source and target images.  </li> <li>  Step 2:   Replace the low frequency part of the source amplitude with that of the target.  </li> <li>  Step 3:   Apply the inverse FFT to the modified source spectrum.  </li> </ul> <ul> <li> For specific steps to make this work with the FDA, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#styleflow-for-content-fixed-image-to-image-translation-styleflow","title":"StyleFlow For Content-Fixed Image to Image Translation (StyleFlow)","text":"<p> With the invertible network structure, StyleFlow first projects the input images into the feature space in the forward, while the backward uses the SAN module to perform the fixed feature transformation of the content, and then projects them into image space.   The blue arrows indicate the forward pass to extract the features, while the red arrows represent the backward pass to reconstruct the images. StyleFlow consists of a series of reversible blocks, where each block has three components: the <code>Squeeze module</code>, the <code>Flow module</code>, and the <code>SAN module</code>. A pre-trained VGG encoder is used for domain feature extraction. </p> <ul> <li>  Squeeze module:   The Squeeze operation serves as an interconnection between blocks for reordering features. It reduces the spatial size of the feature map by first dividing the input feature into small patches along the spatial dimension and then concatenating the patches along the channel dimension.  </li> <li>  Flow module:   The Flow module consists of three reversible transformations: Actnorm Layer, 1x1 Convolution Layer, and Coupling Layer.  </li> <li>  SAN module:   SAN module to perform fixed content feature transformation. Fixed content transfer means that content information before and after transformation should be retained.  </li> </ul> <ul> <li> For specific steps to make this work with the StyleFlow model, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#contrastive-coherence-preserving-loss-for-versatile-style-transfer-ccpl","title":"Contrastive Coherence Preserving Loss for Versatile Style Transfer (CCPL)","text":"<p> Another method to do style change is to use CCPL model.   Inspirations for CCPL:   Regions denoted by red boxes from the first frame <code>(RA or R'A)</code> have the same location with corresponding patches in the second frame wrapped in a yellow box <code>(RB or R'B)</code>. <code>RC and R'C</code> (in the blue boxes) are cropped from the first frame but their style aligns with <code>RB and R'B</code>. The difference between two patches is denoted by <code>D</code> (for example, D(RA, RB)). Mutual information between <code>D(RA, RC)</code> and <code>D(R'A, R'C)</code>, <code>(D(RA, RB) and D(R'A, R'B))</code> is encouraged to be maximized to preserve consistency from the content source.   Details of CCPL:  <code>Cf</code> and <code>Gf</code> represent the encoded features of a specific layer of encoder <code>E</code>. <code>\u2296</code> denotes vector subtraction, and <code>SCE</code> stands for softmax cross-entropy. The yellow dotted lines illustrate how the positive pair is produced. </p> <ul> <li> For specific steps to make this work with the CCPL model, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#gan","title":"GAN","text":"<p> Another approach that should be tested is GAN models. Here we take back the DCGAN model available in Augmentare for testing. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#deep-convolutional-gan-dcgan","title":"Deep Convolutional GAN (DCGAN)","text":"<p> To generate new images by DCGAN model, we first trained this model on normal highway image set and then generated new normal highway images. Since the number of blue-veiled highway images in the dataset is too small (30 images) and to save time, we reused the model after training on the normal highway image set and continued to train it on the blue highway image set and then generate new blue highway images. We did the same for the task of generating river images. </p> <ul> <li> For specific steps to make this work with the CCPL model, please see the tutorial here:   View colab tutorial  |   View source  | \ud83d\udcf0 Paper </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#gan-nst","title":"GAN + NST","text":"<p> To obtain better blue images, we combined the DCGAN model and NST models <code>(FDA and Flow)</code>. We basically took the normal images generated by DCGAN and then used NST methods to style these normal images into blue images. Let's see the results below. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#dcgan-fda","title":"DCGAN + FDA","text":"<ul> <li> For specific steps to make this work with the CCPL model, please see the tutorial here:   View colab tutorial  </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#dcgan-flow","title":"DCGAN + Flow","text":"<ul> <li> For specific steps to make this work with the CCPL model, please see the tutorial here:   View colab tutorial  </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#results-of-the-experiments","title":"Results of the experiments","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#setting-up-experiments","title":"Setting up experiments","text":"<p> The next step is to set up trials to evaluate the Data Augmentation approach. We set up our experiments as shown figure below. Basically, we will use the Data Augmentation methods introduced above to generate new blue images, then add them to the original dataset retrain the classification model, and compare the error rates on the test set with the Baseline. There are a few caveats in our experiments: </p> <ul> <li>  First:   When we add blue images to the original dataset, the number of blue highway and blue river images corresponds to a pair of numbers on a matrix as shown in the resulting image in the next section. For example, with the number of added blue highway images `0`, the number of added blue river images will be `0,100,200,300...` so we will have the corresponding pairs of numbers `[0,0], [0,100], [0,200],...` The purpose of doing this is to see how many blue highway and blue river images need to be added to the original dataset to get the best results.  </li> <li>  Second:   Each pair of blue highway and blue river image numbers will be added to the original dataset and retrained the model, it should be noted that as with baseline we also train 10 times with 10 different seeds.  </li> </ul> <p> </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#evaluation","title":"Evaluation","text":"<p> After doing 10 training times with different seed values, we will calculate their average. Let's see the results when we add the new minority images to the original dataset. Below is an example of the FDA method and the Baseline is the ResNet18 model. </p> <ul> <li> <p> For specific steps to do this with the <code>Vgg16</code> model please see the source file and see all the results of the Data Augmentation methods please see notebook:   View colab tutorial  </p> </li> <li> <p> For specific steps to do this with the <code>ResNet18</code> model please see the source file and see all the results of the Data Augmentation methods please see notebook:   View colab tutorial  </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#results","title":"Results","text":"<ul> <li> To see all analysis of the results please see the following notebook:   View colab tutorial  </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#error-rate-of-blue-and-normal-images","title":"Error rate of blue and normal images","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#mean-std","title":"Mean + Std","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#blue-images","title":"Blue images","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#normal-images","title":"Normal images","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#summary","title":"Summary","text":"<ul> <li> <p> In general, the DA methods improve the error rate of the minority group of images that are blue images. However we can see that the CCPL method does not work in this case. </p> </li> <li> <p> After using DA methods the error rate of normal images with ResNet18 increased a bit. For the Vgg16 model, the error rate of the normal image is also improved. </p> </li> <li> <p> The combination of DCGAN + FDA or DCGAN + Flow works better than FDA or Flow on ResNet18 models but not on Vgg16. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#explainable-ai","title":"Explainable AI","text":"<p> Some of the questions raised are: </p> <ul> <li>  Ques 1:   What is the difference in predicting an image between a baseline model and a model after data enhancement?      </li> <li>  Ques 2:   What is the difference between the true and false prediction of the model?  </li> <li>  Ques 3:   How to evaluate them?  </li> </ul> <p> So, to answer these questions we attack XAI methods. Here we use the methods shown below, they are available in the OmniXAI library.  </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#attributions","title":"Attributions","text":"<p> We will first use XAI methods to generate attributes to see where the classification models are based on the input image to predict if it is a highway or a river image. Specifically where the saliency maps will be concentrated in the image, let's see a few examples below. </p> <ul> <li> <p> For specific steps to make this work for <code>ResNet18</code> model, please see the tutorial here:   View colab tutorial  </p> </li> <li> <p> For specific steps to make this work for <code>Vgg16</code> model, please see the tutorial here:   View colab tutorial  </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#resnet18_1","title":"ResNet18","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#vgg16_1","title":"Vgg16","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#xai-metrics","title":"XAI metrics","text":"<p> How to evaluate these XAI methods? We will use the two metrics mentioned below: </p> <ul> <li>  Deletion Metric:   The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. (Lower is better) | \ud83d\udcf0 [Paper](https://arxiv.org/pdf/1806.07421.pdf)      </li> <li>  Insertion Metric:   The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. (Higher is better) | \ud83d\udcf0 [Paper](https://arxiv.org/pdf/1806.07421.pdf)  </li> </ul> <ul> <li> <p> For specific steps to make this work for <code>ResNet18</code> model, please see the tutorial here:   View colab tutorial  </p> </li> <li> <p> For specific steps to make this work for <code>Vgg16</code> model, please see the tutorial here:   View colab tutorial  </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#deletion-and-insertion-score","title":"Deletion and Insertion score","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#deletion-score","title":"Deletion score","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#insertion-score","title":"Insertion score","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#mean-std_1","title":"Mean + Std","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#deletion-score_1","title":"Deletion score","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#insertion-score_1","title":"Insertion score","text":""},{"location":"api/experiments_summary/Bias_EuroSAT/#example-of-deletion-and-insertion-games-baseline","title":"Example of Deletion and Insertion games (Baseline)","text":"<p> To illustrate our comment above let's take a look at some examples below of the Deletion and Insertion games of XAI methods. In fact, the Deletion and Insertion scores are calculated using the <code>Iou</code> formula, i.e. the area of the area below the blue line in the figures below. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#difference-between-baseline-and-augmented-model","title":"Difference between Baseline and Augmented model","text":"<p> In the previous section, we saw that there is a peak at the end of the Deletion line, this is because the model we used is not good. So let's see if augmented models improve this. </p>"},{"location":"api/experiments_summary/Bias_EuroSAT/#summary_1","title":"Summary","text":"<ul> <li> <p> The XAI methods explained the difference between the true and false predictions of the model. </p> </li> <li> <p> They also explain why models with data augmentation have better performance than baseline models. Especially, the combination of GAN and NST achieves the best results. </p> </li> <li> <p> We also find that CAM methods explain Highway images well, while methods like Ig, Guidedbp, Shap explain better with River images. </p> </li> <li> <p> However, there are still a few failure cases, that is, the Deletion scores of some cases are still quite high, especially for River images. This is because during the selection of important features, the XAI methods cannot get rid of the background noise, causing the mask of important pixels to take up almost the entire image to be explained. </p> </li> </ul>"},{"location":"api/experiments_summary/Bias_EuroSAT/#conclusions","title":"Conclusions","text":""},{"location":"api/gan/cdcgan/","title":"Conditional Deep Convolutional GAN (CDCGAN)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p> <p><code>Conditional Deep Convolutional GAN</code> is a conditional GAN that use the same convolution layers as <code>DCGAN</code> that is described previously. <code>CDCGAN</code> generate more realistic images than <code>CGAN</code> thanks to convolutional layers.</p>"},{"location":"api/gan/cdcgan/#network-architecture-cdcgan","title":"NETWORK ARCHITECTURE : CDCGAN","text":""},{"location":"api/gan/cdcgan/#generator-network","title":"GENERATOR NETWORK","text":"<p>The CDCGAN Generator is parameterized to learn and produce realistic samples for each label in the training dataset. It receives an input noise vector of size \\(batch\\ size \\times latent\\ size\\). It outputs a tensor of \\(batch\\ size \\times channel \\times height \\times width\\) corresponding to a batch of generated image samples.</p> <p>The intermediate layers use the ReLU activation function to kill gradients and slow down convergence. We can also use any other activation to ensure a good gradation flow. The last layer uses the Tanh activation to constrain the pixel values \u200b\u200bto the range of \\((- 1 \\to 1)\\). </p>"},{"location":"api/gan/cdcgan/#discriminator-network","title":"DISCRIMINATOR NETWORK","text":"<p>The CDCGAN Discriminator learns to distinguish fake and real samples, given the label information. It has a symmetric architecture to the generator. It maps the image with a confidence score to classify whether the image is real (i.e. comes from the dataset) or fake (i.e. sampled by the generator) </p> <p>We use the LeakyReLU activation for Discriminator.</p> <p>The last layer of CDCGAN's Discriminator has a Sigmoid layer that makes the confidence score between \\((0 \\to 1)\\) and allows the confidence score to be easily interpreted in terms of the probability that the image is real. However, this interpretation is restricted only to the Minimax Loss proposed in the original GAN paper, and losses such as the Wasserstein Loss require no such interpretation. However, if required, one can easily set last layer activation to Sigmoid by passing it as a parameter during initialization time.</p>"},{"location":"api/gan/cdcgan/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\n\n# Create GAN Generator\nnet_gen = CDCGANGenerator(\n    num_classes=10,\n    latent_size=10,\n    label_embed_size=5,\n    channels=3,\n    conv_dim=64\n)\n\n# Create GAN Discriminator\nnet_dis = CDCGANDiscriminator(\n    num_classes=10,\n    channels=3,\n    conv_dim=64,\n    image_size=image_size\n)\n\n# Optimizers and Loss functions\noptimizer_gen = Adam(net_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_dis = Adam(net_dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\nloss_fn_gen =  nn.BCELoss()\nloss_fn_dis =  nn.BCELoss()\n\n# Create GAN network\ngan = CDCGAN(\n    net_gen,\n    net_dis,\n    optimizer_gen,\n    optimizer_dis,\n    loss_fn_gen,\n    loss_fn_dis,\n    device,\n    latent_size=10,\n    init_weights=False\n)\n\n# Training the CDCGAN network\ngen_losses, dis_losses = gan.train(\n    subset_a=dataloader,\n    num_epochs=200,\n    num_decay_epochs = None,\n    num_classes = None,\n    batch_size=256,\n    subset_b = None\n)\n\n# Sample images from the Generator\nimg_list = gan.generate_samples(\n    nb_samples=32,\n    num_classes=8,\n    real_image_a = None,\n    real_image_b = None\n)\n</code></pre>"},{"location":"api/gan/cdcgan/#notebooks","title":"Notebooks","text":"<ul> <li>CDCGAN: Tutorial</li> </ul>"},{"location":"api/gan/cdcgan/#CDCGAN","title":"<code>CDCGAN</code>","text":"<p>A basic CDCGAN class for generating images. </p>"},{"location":"api/gan/cdcgan/#__init__","title":"<code>__init__(self,  generator:  augmentare.methods.gan.base.BaseGenerator,  discriminator:  augmentare.methods.gan.base.BaseDiscriminator,  optimizer_gen:  torch.optim.optimizer.Optimizer,  optimizer_dis:  torch.optim.optimizer.Optimizer,  loss_fn_gen:  Callable,  loss_fn_dis:  Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,  latent_size:  Optional[int] = None,  init_weights:  bool = True)</code>","text":""},{"location":"api/gan/cdcgan/#generate_samples","title":"<code>generate_samples(self,  nb_samples:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_a=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_b=None)</code>","text":"<p>Sample images from the generator. </p> <p>Return</p> <ul> <li> <p>img_list </p> <ul> <li><p> A list of generated images</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cdcgan/#train","title":"<code>train(self,  subset_a:  Union[, torch.utils.data.dataset.Dataset],  num_epochs:  int,\u00a0\u00a0\u00a0\u00a0\u00a0 num_decay_epochs=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None])</code>","text":"<p>Train both networks and return the losses. </p> <p>Parameters</p> <ul> <li> <p>subset_a            : Union[, torch.utils.data.dataset.Dataset] </p> <ul> <li><p> Torch.tensor or Dataset</p> </li> </ul> </li> <li> <p>num_epochs            : int </p> <ul> <li><p> The number of epochs you want to train your CDCGan</p> </li> </ul> </li> <li> <p>batch_size            : batch_size=typing.Optional[int] </p> <ul> <li><p> Training batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_losses, dis_losses </p> <ul> <li><p> The losses of both the discriminator and generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cdcgan/#train_discriminator","title":"<code>train_discriminator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_labels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size)</code>","text":"<p>Train the discriminator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> <li> <p>real_labels            : real_labels </p> <ul> <li><p> True labels of real samples</p> </li> </ul> </li> <li> <p>batch_size            : batch_size </p> <ul> <li><p> Batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>dis_loss </p> <ul> <li><p> The loss of the discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cdcgan/#train_generator","title":"<code>train_generator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_labels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size)</code>","text":"<p>Train the generator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> <li> <p>real_labels            : real_labels </p> <ul> <li><p> True labels of real samples</p> </li> </ul> </li> <li> <p>batch_size            : batch_size </p> <ul> <li><p> Batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_loss </p> <ul> <li><p> The loss of the generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cdcgan/#CDCGANGenerator","title":"<code>CDCGANGenerator</code>","text":"<p>A generator for mapping a latent space to a sample space. </p>"},{"location":"api/gan/cdcgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latent_size,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 label_embed_size,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 conv_dim)</code>","text":""},{"location":"api/gan/cdcgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cdcgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cdcgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cdcgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cdcgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cdcgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cdcgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CDCGANGenerator. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cdcgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cdcgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cdcgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cdcgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cdcgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cdcgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cdcgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p>"},{"location":"api/gan/cdcgan/#CDCGANDiscriminator","title":"<code>CDCGANDiscriminator</code>","text":"<p>A discriminator for discerning real from generated images. Output activation is Sigmoid. </p>"},{"location":"api/gan/cdcgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 conv_dim,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 image_size)</code>","text":""},{"location":"api/gan/cdcgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cdcgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cdcgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cdcgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cdcgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cdcgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cdcgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CDCGANDiscriminator. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cdcgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cdcgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cdcgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cdcgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cdcgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cdcgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cdcgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cdcgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cdcgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cdcgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cdcgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cdcgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cdcgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Unsupervised Representation Learning With Deep Convolutional Generative Aversarial Networks by Radford &amp; al (2015).</p>"},{"location":"api/gan/cgan/","title":"Conditional GAN (CGAN)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p> <p> <p> Conditional GAN Architecture </p></p>"},{"location":"api/gan/cgan/#network-architecture-cgan","title":"NETWORK ARCHITECTURE : CGAN","text":""},{"location":"api/gan/cgan/#generator-network","title":"GENERATOR NETWORK","text":"<p>The CGAN Generator is parameterized to learn and produce realistic samples for each label in the training dataset. It receives an input noise vector of size \\(batch\\ size \\times latent\\ size\\). It outputs a tensor of \\(batch\\ size \\times channel \\times height \\times width\\) corresponding to a batch of generated image samples.</p> <p>The intermediate layers use the LeakyReLU activation function to kill gradients and slow down convergence. We can also use any other activation to ensure a good gradation flow. The last layer uses the Tanh activation to constrain the pixel values \u200b\u200bto the range of \\((- 1 \\to 1)\\). </p>"},{"location":"api/gan/cgan/#discriminator-network","title":"DISCRIMINATOR NETWORK","text":"<p>The CGAN Discriminator learns to distinguish fake and real samples, given the label information. It has a symmetric architecture to the generator. It maps the image with a confidence score to classify whether the image is real (i.e. comes from the dataset) or fake (i.e. sampled by the generator) </p> <p>We use the LeakyReLU activation for Discriminator. We also use Dropout activation, it's an effective technique for regularization and preventing the co-adaptation of neurons.</p> <p>The last layer of CGAN's Discriminator has a Sigmoid layer that makes the confidence score between \\((0 \\to 1)\\) and allows the confidence score to be easily interpreted in terms of the probability that the image is real. However, this interpretation is restricted only to the Minimax Loss proposed in the original GAN paper, and losses such as the Wasserstein Loss require no such interpretation. However, if required, one can easily set last layer activation to Sigmoid by passing it as a parameter during initialization time.</p>"},{"location":"api/gan/cgan/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\n\n# Create GAN Generator\nnet_gen = CGANGenerator(\n    latent_size=100,\n    num_classes=10,\n    image_shape=image_shape\n)\n\n# Create GAN Discriminator\nnet_dis = CGANDiscriminator(\n    num_classes=10,\n    image_shape=image_shape\n)\n\n# Optimizers and Loss functions\noptimizer_gen = Adam(net_gen.parameters(), lr=0.0001)\noptimizer_dis = Adam(net_dis.parameters(), lr=0.0001)\nloss_fn_gen =  nn.BCELoss()\nloss_fn_dis =  nn.BCELoss()\n\n# Create GAN network\ngan = CGAN(\n    net_gen,\n    net_dis,\n    optimizer_gen,\n    optimizer_dis,\n    loss_fn_gen,\n    loss_fn_dis,\n    device,\n    latent_size=100,\n    init_weights=False\n)\n\n# Training the CGAN network\ngen_losses, dis_losses = gan.train(\n    subset_a=dataloader,\n    num_epochs=20,\n    num_decay_epochs = None,\n    num_classes=10,\n    batch_size=32,\n    subset_b = None\n)\n\n# Sample images from the Generator\nimg_list = gan.generate_samples(\n    nb_samples=32,\n    num_classes=8,\n    real_image_a = None,\n    real_image_b = None\n)\n</code></pre>"},{"location":"api/gan/cgan/#notebooks","title":"Notebooks","text":"<ul> <li>CGAN: Tutorial</li> </ul>"},{"location":"api/gan/cgan/#CGAN","title":"<code>CGAN</code>","text":"<p>A basic CGAN class for generating images with the condition. </p>"},{"location":"api/gan/cgan/#__init__","title":"<code>__init__(self,  generator:  augmentare.methods.gan.base.BaseGenerator,  discriminator:  augmentare.methods.gan.base.BaseDiscriminator,  optimizer_gen:  torch.optim.optimizer.Optimizer,  optimizer_dis:  torch.optim.optimizer.Optimizer,  loss_fn_gen:  Callable,  loss_fn_dis:  Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,  latent_size:  Optional[int] = None,  init_weights:  bool = True)</code>","text":""},{"location":"api/gan/cgan/#generate_samples","title":"<code>generate_samples(self,  nb_samples:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_a=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_b=None)</code>","text":"<p>Sample images from the generator. </p> <p>Return</p> <ul> <li> <p>img_list </p> <ul> <li><p> A list of generated images (one image of one class is successively a list)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cgan/#train","title":"<code>train(self,  subset_a:  Union[, torch.utils.data.dataset.Dataset],  num_epochs:  int,\u00a0\u00a0\u00a0\u00a0\u00a0 num_decay_epochs=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None])</code>","text":"<p>Train both networks and return the losses. </p> <p>Parameters</p> <ul> <li> <p>subset_a            : Union[, torch.utils.data.dataset.Dataset] </p> <ul> <li><p> Torch.tensor or Dataset</p> </li> </ul> </li> <li> <p>num_epochs            : int </p> <ul> <li><p> The number of epochs you want to train your CGan</p> </li> </ul> </li> <li> <p>num_classes            : num_classes=typing.Optional[int] </p> <ul> <li><p> Number of classes in dataset</p> </li> </ul> </li> <li> <p>batch_size            : batch_size=typing.Optional[int] </p> <ul> <li><p> Training batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_losses, dis_losses </p> <ul> <li><p> The losses of both the generator and discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cgan/#train_discriminator","title":"<code>train_discriminator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size)</code>","text":"<p>Train the discriminator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> <li> <p>num_classes            : num_classes </p> <ul> <li><p> Number of classes in dataset</p> </li> </ul> </li> <li> <p>batch_size            : batch_size </p> <ul> <li><p> Training batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>dis_loss </p> <ul> <li><p> The loss of the discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cgan/#train_generator","title":"<code>train_generator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size)</code>","text":"<p>Train the generator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>num_classes            : num_classes </p> <ul> <li><p> Number of classes in dataset</p> </li> </ul> </li> <li> <p>batch_size            : batch_size </p> <ul> <li><p> Training batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_loss </p> <ul> <li><p> The loss of the generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cgan/#CGANGenerator","title":"<code>CGANGenerator</code>","text":"<p>A generator is parameterized to learn and produce realistic samples for each label in the training dataset. </p>"},{"location":"api/gan/cgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latent_size,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 image_shape)</code>","text":""},{"location":"api/gan/cgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CGANGenerator. </p> <p></p>"},{"location":"api/gan/cgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p>"},{"location":"api/gan/cgan/#CGANDiscriminator","title":"<code>CGANDiscriminator</code>","text":"<p>A discriminator learns to distinguish fake and real samples, given the label information. Output activation is Sigmoid. </p>"},{"location":"api/gan/cgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 image_shape)</code>","text":""},{"location":"api/gan/cgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CGANDiscriminator. </p> <p></p>"},{"location":"api/gan/cgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Conditional Generative Adversarial Nets by Mehdi Mirza &amp; Simon Osindero (2014).</p>"},{"location":"api/gan/cyclegan/","title":"Cycle GAN","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p> <p>Have you ever had the dark secret of turning a horse into a zebra? CycleGAN was developed to do just that. Learn how to turn a horse into a zebra and much more.</p> <p>Back to the story of the dark secret, how are we going to do that?</p> <p><code>We'll climb over the neighbor's fence in the middle of the night to paint a horse with stripes. Snap a snap of a horse before we start our fucking act. Then paint the horses quickly before their owners notice. Take a photo of the zebra-striped horse's output, then run to the fence before the homeowner's pit bulls spot you. And you'll have to keep doing that over and over again until you have enough example items in your database to train your neural network.</code></p> <p>Forget it because you have CycleGAN. You will build a generator like the Pix2Pix architecture, which the GAN will train to be a Generator to turn a horse into a zebra. And then you build a Generator (again based on the Pix2Pix architecture) for a second inverse GAN that is supposed to take a picture of a zebra and turn it into an image of a horse.</p> <p>Let's look at the image below, which shows the first half of CycleGAN trying to create a fake zebra from a horse. The second half of CycleGAN tries to create a fake horse from a zebra. Both halves include loss of cyclic consistency trying to make the output of the inverting generator match the input of the non-inverting generator.</p> <p> <p> </p></p> <p> <p> Simplified view of CycleGAN architecture  (Image source)  </p></p>"},{"location":"api/gan/cyclegan/#network-architecture-cyclegan","title":"NETWORK ARCHITECTURE : CycleGAN","text":""},{"location":"api/gan/cyclegan/#generator-network","title":"GENERATOR NETWORK","text":"<p>The CycleGAN Generator has 3 components:</p> <ol> <li>A downsampling network: It is composed of 3 convolutional layers  (together with the regular padding, normalization and activation layers).</li> <li>A chain of residual networks built using the Residual Block. You can try to vary the <code>ResidualBlock</code> parameter and see the results.</li> <li>A upsampling network: It is composed of 3 transposed convolutional layers.</li> </ol> <p>In CycleGAN Generator, we shall be using Instance Norm instead of Batch Norm and finally swap the Zero Padding of the Convolutional Layer with Reflection Padding.</p>"},{"location":"api/gan/cyclegan/#discriminator-network","title":"DISCRIMINATOR NETWORK","text":"<p>The CycleGAN Discriminator is like the standard DCGAN Discriminator. The only difference is the Instance Normalization used.</p>"},{"location":"api/gan/cyclegan/#loss-functions","title":"LOSS FUNCTIONS","text":"<p>The Generator Loss is composed of 3 parts. They are described below:</p> <ol> <li>GAN Loss: It is the standard generator loss of the Least Squares GAN. We use the functional forms of the losses to implement this part. \\(\\(L_{GAN} = \\frac{1}{2} \\times ((D_A(G_{B2A}(Image_B)) - 1)^2 + (D_B(G_{A2B}(Image_A)) - 1)^2)\\)\\)</li> <li>Identity Loss: It computes the similarity of a real image of type B and a fake image B generated from image A and vice versa. The similarity is measured using the \\(L_1\\) Loss. \\(\\(L_{identity} = \\frac{1}{2} \\times (||G_{B2A}(Image_B) - Image_A||_1 + ||G_{A2B}(Image_A) - Image_B||_1)\\)\\)</li> <li>Cycle Consistency Loss: This loss computes the similarity of the original image and the image generated by a composition of the 2 generators. This allows cyclegan to deak with unpaired images. We reconstruct the original image and try to minimize the \\(L_1\\) norm between the original images and this reconstructed image. \\(\\(L_{cycle\\_consistency} = \\frac{1}{2} \\times (||G_{B2A}(G_{A2B}(Image_A)) - Image_A||_1 + ||G_{A2B}(G_{B2A}(Image_B)) - Image_B||_1)\\)\\)</li> </ol> <p>The Discriminator as mentioned before is same as the normal DCGAN Discriminator. As such even the loss function for that is same as that of the standard GAN:</p> \\[L_{GAN} = \\frac{1}{2} \\times (((D_A(Image_A) - 1)^2 - (D_A(G_{B2A}(Image_B))^2) + ((D_B(Image_B) - 1)^2 - (D_B(G_{A2B}(Image_A))^2))\\]"},{"location":"api/gan/cyclegan/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\n\n# Create GAN Generator\nnet_gen = CYCLEGANGenerator()\n\n# Create GAN Discriminator\nnet_dis = CYCLEGANDiscriminator()\n\n# Optimizers and Loss functions\noptimizer_gen = Adam(net_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_dis = Adam(net_dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\nloss_fn_gen =  nn.L1Loss()\nloss_fn_dis =  nn.L1Loss()\n\n# Create GAN network\ngan = CYCLEGAN(\n    net_gen,\n    net_dis,\n    optimizer_gen,\n    optimizer_dis,\n    loss_fn_gen,\n    loss_fn_dis,\n    device,\n    latent_size=None\n)\n\n# Training the CycleGAN network\ngen_losses, dis_losses = gan.train(\n    subset_a=data[\"A\"],\n    num_epochs=30,\n    num_decay_epochs=15,\n    num_classes = None,\n    batch_size = None,\n    subset_b = data[\"B\"]\n)\n\n# Sample images from the Generator\nreal_a = data[\"A\"][:36]\nreal_b = data[\"B\"][:36]\n\nfake_image_a, fake_image_b= gan.generate_samples(\n    nb_samples = None,\n    num_classes = None,\n    real_image_a = real_a,\n    real_image_b = real_b\n)\n</code></pre>"},{"location":"api/gan/cyclegan/#notebooks","title":"Notebooks","text":"<ul> <li>CycleGAN: Tutorial</li> <li>CycleGAN: Apply in CelebA</li> </ul>"},{"location":"api/gan/cyclegan/#CYCLEGAN","title":"<code>CYCLEGAN</code>","text":"<p>A basic CycleGAN class for training of image-to-image translation model without paired examples. </p>"},{"location":"api/gan/cyclegan/#__init__","title":"<code>__init__(self,  generator:  augmentare.methods.gan.cyclegan.CYCLEGANGenerator,  discriminator:  augmentare.methods.gan.cyclegan.CYCLEGANDiscriminator,  optimizer_gen:  torch.optim.optimizer.Optimizer,  optimizer_dis:  torch.optim.optimizer.Optimizer,  loss_fn_gen:  Callable,  loss_fn_dis:  Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latent_size=typing.Optional[int])</code>","text":""},{"location":"api/gan/cyclegan/#generate_samples","title":"<code>generate_samples(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nb_samples=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_a=typing.Optional[],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_b=typing.Optional[])</code>","text":"<p>Sample images from the generator. </p> <p>Return</p> <ul> <li> <p>fake_image_a </p> <ul> <li><p> A list of generated images A</p> </li> </ul> </li> <li> <p>fake_image_b </p> <ul> <li><p> A list of generated images B</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cyclegan/#train","title":"<code>train(self,  subset_a:  Union[, torch.utils.data.dataset.Dataset],  num_epochs:  int,\u00a0\u00a0\u00a0\u00a0\u00a0 num_decay_epochs=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None])</code>","text":"<p>The corresponding training function </p> <p>Parameters</p> <ul> <li> <p>subset_a            : Union[, torch.utils.data.dataset.Dataset] </p> <ul> <li><p> Torch.tensor or Dataset</p> </li> </ul> </li> <li> <p>num_epochs            : int </p> <ul> <li><p> The number of epochs you want to train your CycleGAN</p> </li> </ul> </li> <li> <p>num_decay_epochs            : num_decay_epochs=typing.Optional[int] </p> <ul> <li><p> The number of epochs to start linearly decaying the learning rate to 0</p> </li> </ul> </li> <li> <p>subset_b            : subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None] </p> <ul> <li><p> The second Torch.tensor or Dataset</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_losses, dis_losses </p> <ul> <li><p> The losses of both the discriminator and generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cyclegan/#train_discriminator","title":"<code>train_discriminator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples_a,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples_b)</code>","text":"<p>Train the discriminator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples_a            : real_samples_a </p> <ul> <li><p> True samples of your dataset A</p> </li> </ul> </li> <li> <p>real_samples_b            : real_samples_b </p> <ul> <li><p> True samples of your dataset B</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>dis_loss </p> <ul> <li><p> The loss of the discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cyclegan/#train_generator","title":"<code>train_generator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples_a,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples_b)</code>","text":"<p>Train the generator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples_a            : real_samples_a </p> <ul> <li><p> True samples of your dataset A</p> </li> </ul> </li> <li> <p>real_samples_b            : real_samples_b </p> <ul> <li><p> True samples of your dataset B</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_loss </p> <ul> <li><p> The loss of the generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/cyclegan/#CYCLEGANGenerator","title":"<code>CYCLEGANGenerator</code>","text":"<p>A conditional generator for synthesizing an image given an input image. </p>"},{"location":"api/gan/cyclegan/#__init__","title":"<code>__init__(self)</code>","text":""},{"location":"api/gan/cyclegan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cyclegan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cyclegan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cyclegan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cyclegan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cyclegan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cyclegan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CYCLEGANGenerator. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cyclegan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cyclegan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cyclegan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cyclegan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cyclegan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cyclegan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cyclegan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p>"},{"location":"api/gan/cyclegan/#CYCLEGANDiscriminator","title":"<code>CYCLEGANDiscriminator</code>","text":"<p>A discriminator for predicting how likely the generated image is to have come from the target image collection. </p>"},{"location":"api/gan/cyclegan/#__init__","title":"<code>__init__(self)</code>","text":""},{"location":"api/gan/cyclegan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/cyclegan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/cyclegan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/cyclegan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/cyclegan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/cyclegan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/cyclegan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function CYCLEGANDiscriminator. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/cyclegan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/cyclegan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/cyclegan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/cyclegan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/cyclegan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/cyclegan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/cyclegan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/cyclegan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/cyclegan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/cyclegan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/cyclegan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/cyclegan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/cyclegan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks by Jun-Yan Zhu &amp; al (2017).</p>"},{"location":"api/gan/dcgan/","title":"Deep Convolutional GAN (DCGAN)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p> <p> <p> DCGAN Generator </p></p> <p> <p> Full DCGAN Architecture </p></p>"},{"location":"api/gan/dcgan/#network-architecture-dcgan","title":"NETWORK ARCHITECTURE : DCGAN","text":""},{"location":"api/gan/dcgan/#generator-network","title":"GENERATOR NETWORK","text":"<p>The DCGAN Generator receives an input noise vector of size \\(batch\\ size \\times latent\\ size\\). It outputs a tensor of \\(batch\\ size \\times channel \\times height \\times width\\) corresponding to a batch of generated image samples. The generator transforms the noise vector into images in the following manner:</p> <ol> <li>Channel Dimension: \\(encoding\\ dims \\rightarrow d \\rightarrow \\frac{d}{2} \\rightarrow \\frac{d}{4} \\rightarrow \\frac{d}{8} \\rightarrow 1\\).</li> <li>Image size: \\((1 \\times 1) \\rightarrow (4 \\times 4) \\rightarrow (8 \\times 8) \\rightarrow (16 \\times 16) \\rightarrow (32 \\times 32) \\rightarrow (64 \\times 64)\\).</li> </ol> <p>The intermediate layers use the ReLU activation function to kill gradients and slow down convergence. We can also use any other activation to ensure a good gradation flow. The last layer uses the Tanh activation to constrain the pixel values \u200b\u200bto the range of \\((- 1 \\to 1)\\). We can easily change the nonlinearity of the intermediate and last layers to their preferences by passing them as parameters during Generator object initialization. </p>"},{"location":"api/gan/dcgan/#discriminator-network","title":"DISCRIMINATOR NETWORK","text":"<p>The DCGAN Discriminator has a symmetric architecture to the generator. It maps the image with a confidence score to classify whether the image is real (i.e. comes from the dataset) or fake (i.e. sampled by the generator) </p> <p>We use the Leaky ReLU activation for Discriminator. The conversion of image tension to confidence score is as follows: </p> <ol> <li>Channel Dimension: \\(1 \\rightarrow d \\rightarrow 2 \\times d \\rightarrow 4 \\times d \\rightarrow 8 \\times d \\rightarrow 1\\).</li> <li>Image size: \\((64 \\times 64) \\rightarrow (32 \\times 32) \\rightarrow (16 \\times 16) \\rightarrow (8 \\times 8) \\rightarrow (4 \\times 4) \\rightarrow (1 \\times 1)\\).</li> </ol> <p>The last layer of DCGAN's Discriminator has a Sigmoid layer that makes the confidence score between \\((0 \\to 1)\\) and allows the confidence score to be easily interpreted in terms of the probability that the image is real. However, this interpretation is restricted only to the Minimax Loss proposed in the original GAN paper, and losses such as the Wasserstein Loss require no such interpretation. However, if required, one can easily set last layer activation to Sigmoid by passing it as a parameter during initialization time.</p>"},{"location":"api/gan/dcgan/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\n\n# Create GAN Generator\nnet_gen = DCGANGenerator(\n    num_channels=1,\n    latent_size=100,\n    feature_map_size=64\n)\n\n# Create GAN Discriminator\nnet_dis = DCGANDiscriminator(\n    num_channels=1,\n    feature_map_size=64\n)\n\n# Optimizers and Loss functions\noptimizer_gen = Adam(net_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_dis = Adam(net_dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\nloss_fn_gen =  nn.BCELoss()\nloss_fn_dis =  nn.BCELoss()\n\n# Create GAN network\ngan = DCGAN(\n    net_gen,\n    net_dis,\n    optimizer_gen,\n    optimizer_dis,\n    loss_fn_gen,\n    loss_fn_dis,\n    device,\n    latent_size=100,\n    init_weights=True\n)\n\n# Training the DCGAN network\ngen_losses, dis_losses = gan.train(\n    subset_a=dataloader,\n    num_epochs=10,\n    num_decay_epochs = None,\n    num_classes = None,\n    batch_size = None,\n    subset_b = None \n)\n\n# Sample images from the Generator\nimg_list = gan.generate_samples(\n    nb_samples=64,\n    num_classes = None,\n    real_image_a = None,\n    real_image_b = None\n)\n</code></pre>"},{"location":"api/gan/dcgan/#notebooks","title":"Notebooks","text":"<ul> <li>DCGAN: Tutorial</li> <li>DCGAN: Apply in EuroSAT</li> <li>DCGAN: Apply in CelebA</li> </ul>"},{"location":"api/gan/dcgan/#DCGAN","title":"<code>DCGAN</code>","text":"<p>A basic DCGAN class for generating images. </p>"},{"location":"api/gan/dcgan/#__init__","title":"<code>__init__(self,  generator:  augmentare.methods.gan.base.BaseGenerator,  discriminator:  augmentare.methods.gan.base.BaseDiscriminator,  optimizer_gen:  torch.optim.optimizer.Optimizer,  optimizer_dis:  torch.optim.optimizer.Optimizer,  loss_fn_gen:  Callable,  loss_fn_dis:  Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,  latent_size:  Optional[int] = None,  init_weights:  bool = True)</code>","text":""},{"location":"api/gan/dcgan/#generate_samples","title":"<code>generate_samples(self,  nb_samples:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_a=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_b=None)</code>","text":"<p>Sample images from the generator. </p> <p>Return</p> <ul> <li> <p>img_list </p> <ul> <li><p> A list of generated images</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/dcgan/#train","title":"<code>train(self,  subset_a:  Union[, torch.utils.data.dataset.Dataset],  num_epochs:  int,\u00a0\u00a0\u00a0\u00a0\u00a0 num_decay_epochs=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None])</code>","text":"<p>Train both networks and return the losses. </p> <p>Parameters</p> <ul> <li> <p>subset_a            : Union[, torch.utils.data.dataset.Dataset] </p> <ul> <li><p> Torch.tensor or Dataset</p> </li> </ul> </li> <li> <p>num_epochs            : int </p> <ul> <li><p> The number of epochs you want to train your DCGan</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_losses, dis_losses </p> <ul> <li><p> The losses of both the discriminator and generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/dcgan/#train_discriminator","title":"<code>train_discriminator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples)</code>","text":"<p>Train the discriminator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>dis_loss </p> <ul> <li><p> The loss of the discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/dcgan/#train_generator","title":"<code>train_generator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples)</code>","text":"<p>Train the generator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_loss </p> <ul> <li><p> The loss of the generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/dcgan/#DCGANGenerator","title":"<code>DCGANGenerator</code>","text":"<p>A generator for mapping a latent space to a sample space. </p>"},{"location":"api/gan/dcgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latent_size,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 feature_map_size)</code>","text":""},{"location":"api/gan/dcgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/dcgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/dcgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/dcgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/dcgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/dcgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/dcgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/dcgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/dcgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function DCGANGenerator. </p> <p></p>"},{"location":"api/gan/dcgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/dcgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/dcgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/dcgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/dcgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/dcgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/dcgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/dcgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/dcgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/dcgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/dcgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/dcgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/dcgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/dcgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/dcgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p>"},{"location":"api/gan/dcgan/#DCGANDiscriminator","title":"<code>DCGANDiscriminator</code>","text":"<p>A discriminator for discerning real from generated images. Output activation is Sigmoid. </p>"},{"location":"api/gan/dcgan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 feature_map_size)</code>","text":""},{"location":"api/gan/dcgan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/dcgan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/dcgan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/dcgan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/dcgan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/dcgan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/dcgan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/dcgan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/dcgan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=typing.Optional[])</code>","text":"<p>A forward function DCGANDiscriminator. </p> <p></p>"},{"location":"api/gan/dcgan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/dcgan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/dcgan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/dcgan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/dcgan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/dcgan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/dcgan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/dcgan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/dcgan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/dcgan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/dcgan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/dcgan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/dcgan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/dcgan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/dcgan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/dcgan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/dcgan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/dcgan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/dcgan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/dcgan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Unsupervised Representation Learning With Deep Convolutional Generative Aversarial Networks by Radford &amp; al (2015).</p>"},{"location":"api/gan/progan/","title":"Progressive Growing of GANS (ProGAN)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p> <p>Progressive Growing GAN also known as ProGAN is an extension of the GAN training process that allows training generating models with stability that can produce large-high-quality images.</p> <p>It involves training by starting with a very small image and then layer blocks are added gradually so that the output size of the generator model increases and the input size of the discriminator model increases until the desired image size is obtained. This approach has proven to be very effective in creating highly realistic, high-quality synthetic images.</p> <p>It basically includes 4 steps: - Progressive growing (of model and layers)</p> <ul> <li> <p>Minibatch std on Discriminator</p> </li> <li> <p>Normalization with PixelNorm</p> </li> <li> <p>Equalized Learning Rate</p> </li> </ul> <p> <p> Simplified view of ProGAN  (Image source)  </p></p> <p>Here we can see in the above figure that Progressive Growing GAN involves using a generator and discriminator model with the traditional GAN structure and its starts with very small images, such as 4\u00d74 pixels.</p> <p>During training, it systematically adds new convolutional blocks to both the generator model and the discriminator model. This gradual addition of convolutional layers allows models to effectively learn coarse-level details early on and then learn even finer details, both on the generator and discriminator.</p> <p>ProGAN goals: - Produce high-quality, high-resolution images. - Greater diversity of images in the output. - Improve stability in GANs. - Increase variation in the generated images</p>"},{"location":"api/gan/progan/#network-architecture-progan","title":"NETWORK ARCHITECTURE : ProGAN","text":""},{"location":"api/gan/progan/#generator-network","title":"GENERATOR NETWORK","text":"<p>A generator to incrementally size the output by starting with a very small image, then the blocks of layers added incrementally and increasing the input size of the discriminant model until the desired image size is obtained.</p>"},{"location":"api/gan/progan/#discriminator-network","title":"DISCRIMINATOR NETWORK","text":"<p>A discriminator for discerning real from generated images.</p>"},{"location":"api/gan/progan/#loss-functions","title":"LOSS FUNCTIONS","text":"<p>ProGAN use one of the common loss functions in GANs, the Wasserstein loss function, also known as WGAN-GP from the paper Improved Training of Wasserstein GANs. </p> \\[Loss_{G} = -D(x')$$ $$GP = (||\\nabla D(ax' + (1-a)x))||_2 - 1)^2$$ $$Loss_{D} = -D(x) + D(x') + \\lambda * GP\\] <p>Where: - x' is the generated image. - x is an image from the training set. - D is the discriminator. - GP is a gradient penalty that helps stabilize training. - The a term in the gradient penalty refers to a tensor of random numbers between 0 and 1, chosen uniformly at random. - The parameter \u03bb is common to set to 10.</p>"},{"location":"api/gan/progan/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.gan import *\n\n# Create GAN Generator\nnet_gen = PROGANGenerator(\n    latent_size=128,\n    in_channels=128,\n    img_channels=3,\n    alpha=1e-5,\n    steps=4\n)\n\n# Create GAN Discriminator\nnet_dis = PROGANDiscriminator(\n    in_channels=128,\n    img_channels=3,\n    alpha=1e-5,\n    steps=4\n)\n\n# Optimizers and Loss functions\noptimizer_gen = Adam(net_gen.parameters(), lr=1e-3, betas=(0.0, 0.999))\noptimizer_dis = Adam(net_dis.parameters(), lr=1e-3, betas=(0.0, 0.999))\nloss_fn_gen =  torch.cuda.amp.GradScaler()\nloss_fn_dis =  torch.cuda.amp.GradScaler()\n\n# Create GAN network\ngan = PROGAN(\n    net_gen,\n    net_dis,\n    optimizer_gen,\n    optimizer_dis,\n    loss_fn_gen,\n    loss_fn_dis,\n    device,\n    latent_size=128\n)\n\n# Training the ProGAN network\ngen_losses, dis_losses = gan.train(\n    subset_a=dataloader,\n    num_epochs=5,\n    num_decay_epochs=None,\n    num_classes = None,\n    batch_size = [32, 32, 32, 16, 16, 16, 16, 8, 4],\n    subset_b = None\n)\n\n# Sample images from the Generator\nimg_list = gan.generate_samples(\n    nb_samples = 36,\n    num_classes = None,\n    real_image_a = None,\n    real_image_b = None\n)\n</code></pre>"},{"location":"api/gan/progan/#notebooks","title":"Notebooks","text":"<ul> <li>CycleGAN: Tutorial</li> <li>CycleGAN: Apply in CelebA</li> </ul>"},{"location":"api/gan/progan/#PROGAN","title":"<code>PROGAN</code>","text":"<p>A basic ProGAN class for synthesizing high resolution and high quality images via the incremental growing of the discriminator and the generator networks during the training process. </p>"},{"location":"api/gan/progan/#__init__","title":"<code>__init__(self,  generator:  augmentare.methods.gan.base.BaseGenerator,  discriminator:  augmentare.methods.gan.base.BaseDiscriminator,  optimizer_gen:  torch.optim.optimizer.Optimizer,  optimizer_dis:  torch.optim.optimizer.Optimizer,  loss_fn_gen:  Callable,  loss_fn_dis:  Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,  latent_size:  Optional[int] = None)</code>","text":""},{"location":"api/gan/progan/#generate_samples","title":"<code>generate_samples(self,  nb_samples:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_a=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_image_b=None)</code>","text":"<p>Sample images from the generator. </p> <p>Return</p> <ul> <li> <p>img_list </p> <ul> <li><p> A list of generated images</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/progan/#train","title":"<code>train(self,  subset_a:  Union[, torch.utils.data.dataset.Dataset],  num_epochs:  int,\u00a0\u00a0\u00a0\u00a0\u00a0 num_decay_epochs=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 num_classes=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=typing.Optional[int],\u00a0\u00a0\u00a0\u00a0\u00a0 subset_b=typing.Union[, torch.utils.data.dataset.Dataset, None])</code>","text":"<p>Train both networks and return the losses. </p> <p>Parameters</p> <ul> <li> <p>subset_a            : Union[, torch.utils.data.dataset.Dataset] </p> <ul> <li><p> Torch.tensor or Dataset</p> </li> </ul> </li> <li> <p>num_epochs            : int </p> <ul> <li><p> The number of epochs you want to train your ProGAN</p> </li> </ul> </li> <li> <p>batch_size            : batch_size=typing.Optional[int] </p> <ul> <li><p> Training batch size</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_losses, dis_losses </p> <ul> <li><p> The losses of both the discriminator and generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/progan/#train_discriminator","title":"<code>train_discriminator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 real_samples,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise)</code>","text":"<p>Train the discriminator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>real_samples            : real_samples </p> <ul> <li><p> True samples of your dataset</p> </li> </ul> </li> <li> <p>noise            : noise </p> <ul> <li><p> Noise for train discriminator</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>dis_loss </p> <ul> <li><p> The loss of the discriminator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/progan/#train_generator","title":"<code>train_generator(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise)</code>","text":"<p>Train the generator one step and return the loss. </p> <p>Parameters</p> <ul> <li> <p>noise            : noise </p> <ul> <li><p> Noise for train generator</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>gen_loss </p> <ul> <li><p> The loss of the generator</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/gan/progan/#PROGANGenerator","title":"<code>PROGANGenerator</code>","text":"<p>A generator to incrementally size the output by starting with a very small image, then the blocks of layers added incrementally and increasing the input size of the discriminant model until the desired image size is obtained. </p>"},{"location":"api/gan/progan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latent_size,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 in_channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 img_channels=3,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1e-05,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 steps=4)</code>","text":""},{"location":"api/gan/progan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/progan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/progan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/progan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/progan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/progan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/progan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/progan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/progan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/progan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=None)</code>","text":"<p>A forward function PROGANGenerator. </p> <p></p>"},{"location":"api/gan/progan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/progan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/progan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/progan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/progan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/progan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/progan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/progan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/progan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/progan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/progan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/progan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/progan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/progan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/progan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/progan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/progan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/progan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/progan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/progan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/progan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/progan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/progan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/progan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p>"},{"location":"api/gan/progan/#PROGANDiscriminator","title":"<code>PROGANDiscriminator</code>","text":"<p>A discriminator for discerning real from generated images. </p>"},{"location":"api/gan/progan/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 in_channels,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 img_channels=3,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1e-05,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 steps=4)</code>","text":""},{"location":"api/gan/progan/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/gan/progan/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/gan/progan/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/gan/progan/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/gan/progan/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/gan/progan/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/gan/progan/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/gan/progan/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/gan/progan/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/gan/progan/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#forward","title":"<code>forward(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 noise,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 labels=None)</code>","text":"<p>A forward function PROGANDiscriminator. </p> <p></p>"},{"location":"api/gan/progan/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/gan/progan/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/gan/progan/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/gan/progan/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/gan/progan/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/gan/progan/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/gan/progan/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/gan/progan/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/progan/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/gan/progan/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/gan/progan/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/gan/progan/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/gan/progan/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/gan/progan/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/gan/progan/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/gan/progan/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/gan/progan/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/gan/progan/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/gan/progan/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/gan/progan/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/gan/progan/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/gan/progan/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/gan/progan/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/gan/progan/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/gan/progan/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/gan/progan/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/gan/progan/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Progressive Growing of GANs for Improved Quality, Stability, and Variation by Tero Karras &amp; al (2018).</p>"},{"location":"api/style_transfer/adain/","title":"Adaptive Instance Normalization (AdaIN)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p>"},{"location":"api/style_transfer/adain/#network-architecture-adain","title":"NETWORK ARCHITECTURE : AdaIN","text":"<p>They use the first few layers of a fixed VGG-19 network to encode the content and style images. An AdaIN layer is used to perform style transfer in the feature space. A decoder is learned to invert the AdaIN output to the image spaces. They use the same VGG encoder to compute a content loss L<sub>c</sub> and a style loss L<sub>s</sub>.</p> <p></p>"},{"location":"api/style_transfer/adain/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\n\n# Create AdaIN network\nmodel = ADAIN(device)\n\n# Optimizers\noptimizer = Adam(model.parameters(), lr=1e-4)\n\n# Training the AdaIN network\nloss_train = model.train_network(\n            num_epochs=49,\n            train_loader= train_loader,\n            optimizer= optimizer\n        )\n\n# Styled image by AdaIN\ngen_image = model.adain_generate(content_tensor, style_tensor, alpha=1.0)\n</code></pre>"},{"location":"api/style_transfer/adain/#notebooks","title":"Notebooks","text":"<ul> <li>AdaIN: Tutorial</li> <li>AdaIN: Apply in EuroSAT</li> </ul>"},{"location":"api/style_transfer/adain/#ADAIN","title":"<code>ADAIN</code>","text":"<p>Adaptive Instance Normalization (AdaIN) that aligns the mean and variance of the content features with those of the style features. It achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, this approach allows flexible user controls such as content-style trade-off, style interpolation, color &amp; spatial controls, all using a single feed-forward neural network. </p>"},{"location":"api/style_transfer/adain/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device)</code>","text":""},{"location":"api/style_transfer/adain/#adain_generate","title":"<code>adain_generate(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_image,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_image,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1.0)</code>","text":"<p>A function that generates one image after training by AdaIn method. </p> <p></p>"},{"location":"api/style_transfer/adain/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/style_transfer/adain/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/style_transfer/adain/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/adain/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/style_transfer/adain/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/style_transfer/adain/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/style_transfer/adain/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/style_transfer/adain/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/style_transfer/adain/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/adain/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/style_transfer/adain/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/style_transfer/adain/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/adain/#_forward_unimplemented","title":"<code>_forward_unimplemented(self,  *input:  Any) -&gt; None</code>","text":"<p>Defines the computation performed at every call. </p> <p></p>"},{"location":"api/style_transfer/adain/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/adain/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/style_transfer/adain/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/adain/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/adain/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/adain/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/style_transfer/adain/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/style_transfer/adain/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/style_transfer/adain/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/style_transfer/adain/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/adain/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/adain/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/style_transfer/adain/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/style_transfer/adain/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/style_transfer/adain/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/style_transfer/adain/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/style_transfer/adain/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/style_transfer/adain/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/style_transfer/adain/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/style_transfer/adain/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/style_transfer/adain/#train_network","title":"<code>train_network(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_epochs,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 train_loader,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 optimizer,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 lamb=10)</code>","text":"<p>Train the AdaIn network and return the losses. </p> <p></p>"},{"location":"api/style_transfer/adain/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/style_transfer/adain/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/style_transfer/adain/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization by Xuan Huang &amp; Serge Belongie (2017).</p>"},{"location":"api/style_transfer/ccpl/","title":"Contrastive Coherence Preserving Loss for Versatile Style Transfer (CCPL)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p>"},{"location":"api/style_transfer/ccpl/#network-architecture-ccpl","title":"NETWORK ARCHITECTURE : CCPL","text":"<p> Inspirations for CCPL:   Regions denoted by red boxes from the first frame <code>(RA or R'A)</code> have the same location with corresponding patches in the second frame wrapped in a yellow box <code>(RB or R'B)</code>. <code>RC and R'C</code> (in the blue boxes) are cropped from the first frame but their style aligns with <code>RB and R'B</code>. The difference between two patches is denoted by <code>D</code> (for example, D(RA, RB)). Mutual information between <code>D(RA, RC)</code> and <code>D(R'A, R'C)</code>, <code>(D(RA, RB) and D(R'A, R'B))</code> is encouraged to be maximized to preserve consistency from the content source.  </p> <p></p> <p> Details of CCPL:  <code>Cf</code> and <code>Gf</code> represent the encoded features of a specific layer of encoder <code>E</code>. <code>\u2296</code> denotes vector subtraction, and <code>SCE</code> stands for softmax cross-entropy. The yellow dotted lines illustrate how the positive pair is produced. </p>"},{"location":"api/style_transfer/ccpl/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\n\n# Create CCPL method\nvgg_path = '/home/vuong.nguyen/vuong/augmentare/augmentare/methods/style_transfer/model/vgg_normalised_ccpl.pth'\nmodel = CCPL(training_mode= \"pho\", vgg_path=vgg_path, device=device)\n\n# Training the CCPL network\nloss_train = model.train_network(content_images, style_images, num_s=8, num_l=3, max_iter=50000,\n                        content_weight=1.0, style_weight=10.0, ccp_weight=5.0)\n\n# Styled image by CCPL\ngen_image = model.ccpl_generate(\n    content_image, style_image,\n    alpha=1.0, interpolation= False, preserve_color= True\n)\n</code></pre>"},{"location":"api/style_transfer/ccpl/#notebooks","title":"Notebooks","text":"<ul> <li>CCPL: Tutorial</li> <li>CCPL: Apply in EuroSAT</li> </ul>"},{"location":"api/style_transfer/ccpl/#CCPL","title":"<code>CCPL</code>","text":"<p>CCPL class. </p>"},{"location":"api/style_transfer/ccpl/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 training_mode,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 vgg_path,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device)</code>","text":""},{"location":"api/style_transfer/ccpl/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/style_transfer/ccpl/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#ccpl_generate","title":"<code>ccpl_generate(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_images,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 interpolation=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 preserve_color=True)</code>","text":"<p>A function that generates one image after training by CCPL method. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/style_transfer/ccpl/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#_forward_unimplemented","title":"<code>_forward_unimplemented(self,  *input:  Any) -&gt; None</code>","text":"<p>Defines the computation performed at every call. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/style_transfer/ccpl/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#style_transfer","title":"<code>style_transfer(vgg_in,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 decoder_in,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sct_in,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha=1.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 interpolation_weights=None)</code>","text":"<p>Style transfer function for styling the image input. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#train_network","title":"<code>train_network(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_set,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_set,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_s,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 num_l,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_iter,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_weight,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_weight,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ccp_weight)</code>","text":"<p>Train the CCPL network and return the losses. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/style_transfer/ccpl/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Contrastive Coherence Preserving Loss for Versatile Style Transfer by Zijie Wu &amp; al (2022).</p>"},{"location":"api/style_transfer/fda/","title":"Fourier Domain Adaptation (FDA)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p>"},{"location":"api/style_transfer/fda/#network-architecture-fda","title":"NETWORK ARCHITECTURE : FDA","text":"<p>Simplified domain adaptation via style transfer thanks to the Fourier transformation. The FDA does not need deep networks for style transfer and adversarial training.</p> <p></p> <p>The scheme of the proposed Fourier domain adaptation method:     <ul> <li>  Step 1:   Apply FFT to source and target images.  </li> <li>  Step 2:   Replace the low frequency part of the source amplitude with that of the target.  </li> <li>  Step 3:   Apply the inverse FFT to the modified source spectrum.  </li> </ul></p>"},{"location":"api/style_transfer/fda/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\n\n# Create FDA method\nmodel = FDA(im_src, im_trg)\n\n# Styled image by FDA\nsrc_in_trg = model.fda_source_to_target(beta=0.01)\n</code></pre>"},{"location":"api/style_transfer/fda/#notebooks","title":"Notebooks","text":"<ul> <li>FDA: Tutorial</li> <li>FDA: Apply in EuroSAT</li> </ul>"},{"location":"api/style_transfer/fda/#FDA","title":"<code>FDA</code>","text":"<p>Fourier Domain Adaptation is a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. </p>"},{"location":"api/style_transfer/fda/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 source_img,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 target_img)</code>","text":""},{"location":"api/style_transfer/fda/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/style_transfer/fda/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/style_transfer/fda/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/fda/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/style_transfer/fda/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/style_transfer/fda/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/style_transfer/fda/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/style_transfer/fda/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/style_transfer/fda/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/fda/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/style_transfer/fda/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/style_transfer/fda/#fda_source_to_target","title":"<code>fda_source_to_target(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 beta=0.1)</code>","text":"<p>FDA for exchanging magnitude. </p> <p></p>"},{"location":"api/style_transfer/fda/#fda_source_to_target_2","title":"<code>fda_source_to_target_2(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 beta=0.1)</code>","text":"<p>FDA for exchanging magnitude but only take the real fft of source. </p> <p></p>"},{"location":"api/style_transfer/fda/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/fda/#_forward_unimplemented","title":"<code>_forward_unimplemented(self,  *input:  Any) -&gt; None</code>","text":"<p>Defines the computation performed at every call. </p> <p></p>"},{"location":"api/style_transfer/fda/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/fda/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/style_transfer/fda/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/fda/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/fda/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/fda/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/style_transfer/fda/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/style_transfer/fda/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/style_transfer/fda/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/style_transfer/fda/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/fda/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/fda/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/style_transfer/fda/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/style_transfer/fda/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/style_transfer/fda/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/style_transfer/fda/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/style_transfer/fda/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/style_transfer/fda/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/style_transfer/fda/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/style_transfer/fda/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/style_transfer/fda/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/style_transfer/fda/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/style_transfer/fda/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Fourier Domain Adaptation for Semantic Segmentation by Yanchao Yang &amp; Stefano Soatto (2020).</p>"},{"location":"api/style_transfer/nnst/","title":"Neural Neighbor Style Transfer (NNST)","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p>"},{"location":"api/style_transfer/nnst/#network-architecture-nnst","title":"NETWORK ARCHITECTURE : NNST","text":"<p>The fast and slow variants of their method, NNST-D, and NNST-Opt, only differ in step 4; mapping from the target features to image pixels. This simplified diagram omits several details for clarity, namely: they apply steps 1-4 at multiple scales, coarse to fine; they repeat steps 1-4 several times at the finest scale; and they only apply step 5 once (optionally) at the very end.</p>"},{"location":"api/style_transfer/nnst/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\n\n# Create NNST method\nmodel = NNST(content_img, style_img, device)\n\n# Styled image by NNST\ngen_image = model.nnst_generate(\n    max_scales=5, alpha=0.75,\n    content_loss=False, flip_aug=False,\n    zero_init = False, dont_colorize=False\n)\n</code></pre>"},{"location":"api/style_transfer/nnst/#notebooks","title":"Notebooks","text":"<ul> <li>NNST: Tutorial</li> <li>NNST: Apply in EuroSAT</li> </ul>"},{"location":"api/style_transfer/nnst/#NNST","title":"<code>NNST</code>","text":"<p>Neural Neighbor Style Transfer is a pipeline that offers state-of-the-art quality, generalization, and competitive efficiency for artistic style transfer. It's approach is similar to prior work, but it dramatically improve the final visual quality. </p>"},{"location":"api/style_transfer/nnst/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_im,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_im,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device)</code>","text":""},{"location":"api/style_transfer/nnst/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/style_transfer/nnst/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/nnst/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/style_transfer/nnst/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/style_transfer/nnst/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/style_transfer/nnst/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/style_transfer/nnst/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/style_transfer/nnst/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/nnst/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/style_transfer/nnst/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/style_transfer/nnst/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/nnst/#_forward_unimplemented","title":"<code>_forward_unimplemented(self,  *input:  Any) -&gt; None</code>","text":"<p>Defines the computation performed at every call. </p> <p></p>"},{"location":"api/style_transfer/nnst/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/nnst/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/style_transfer/nnst/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/nnst/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/nnst/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/nnst/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/style_transfer/nnst/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/style_transfer/nnst/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/style_transfer/nnst/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/style_transfer/nnst/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/nnst/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/nnst/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/style_transfer/nnst/#nnst_generate","title":"<code>nnst_generate(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_scales,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 alpha,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_loss=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 flip_aug=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 zero_init=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dont_colorize=False)</code>","text":"<p>A function that generates the image by NNST method. </p> <p></p>"},{"location":"api/style_transfer/nnst/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/style_transfer/nnst/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/style_transfer/nnst/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/style_transfer/nnst/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/style_transfer/nnst/#stylization","title":"<code>stylization(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 phi,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_iter=350,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 l_rate=0.001,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_weight=1.0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max_scls=0,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 flip_aug=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_loss=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 zero_init=False,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dont_colorize=False)</code>","text":"<p>Produce stylization of content_im in the style of style_im. </p> <p></p>"},{"location":"api/style_transfer/nnst/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/style_transfer/nnst/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/style_transfer/nnst/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/style_transfer/nnst/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/style_transfer/nnst/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/style_transfer/nnst/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>Neural Neighbor Style Transfer by Nick Kolkin &amp; al (2022).</p>"},{"location":"api/style_transfer/styleflow/","title":"Style Flow","text":"<p>View colab tutorial | View source | \ud83d\udcf0 Paper</p>"},{"location":"api/style_transfer/styleflow/#network-architecture-style-flow","title":"NETWORK ARCHITECTURE : Style Flow","text":"<p>With the invertible network structure, StyleFlow first projects the input images into the feature space in the forward, while the backward uses the SAN module to perform the fixed feature transformation of the content, and then projects them into image space.</p> <p></p> <p>The blue arrows indicate the forward pass to extract the features, while the red arrows represent the backward pass to reconstruct the images. StyleFlow consists of a series of reversible blocks, where each block has three components: the <code>Squeeze module</code>, the <code>Flow module</code>, and the <code>SAN module</code>. A pre-trained VGG encoder is used for domain feature extraction.     <ul> <li>  Squeeze module:   The Squeeze operation serves as an interconnection between blocks for reordering features. It reduces the spatial size of the feature map by first dividing the input feature into small patches along the spatial dimension and then concatenating the patches along the channel dimension.  </li> <li>  Flow module:   The Flow module consists of three reversible transformations: Actnorm Layer, 1x1 Convolution Layer, and Coupling Layer.  </li> <li>  SAN module:   SAN module to perform fixed content feature transformation. Fixed content transfer means that content information before and after transformation should be retained.  </li> </ul></p>"},{"location":"api/style_transfer/styleflow/#example","title":"Example","text":"<pre><code># Augmentare Imports\nimport augmentare\nfrom augmentare.methods.style_transfer import *\n\n# Create StyleFlow method\nvgg_path = '/home/vuong.nguyen/vuong/augmentare/augmentare/methods/style_transfer/model/vgg_normalised_flow.pth'\nmodel = STYLEFLOW(in_channel=3, n_flow=15, n_block=2, vgg_path=vgg_path,\n                            affine=False, conv_lu=False, keep_ratio=0.8, device=device)\n\n# Training the StyleFlow network\nloss_train = model.train_network(train_loader=train_loader,\n            content_weight = 0.1, style_weight=1, type_loss=\"TVLoss\"\n        )\n\n# Styled image by StyleFlow\ngen_image = model.style_flow_generate(\n    content_image= content_image,\n    style_image= style_image\n)\n</code></pre>"},{"location":"api/style_transfer/styleflow/#notebooks","title":"Notebooks","text":"<ul> <li>StyleFlow: Tutorial</li> <li>StyleFlow: Apply in EuroSAT</li> </ul>"},{"location":"api/style_transfer/styleflow/#STYLEFLOW","title":"<code>STYLEFLOW</code>","text":"<p>StyleFlow class. </p>"},{"location":"api/style_transfer/styleflow/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 in_channel,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 n_flow,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 n_block,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 vgg_path,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 affine=True,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 conv_lu=True,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_ratio=0.8,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 device='cpu')</code>","text":""},{"location":"api/style_transfer/styleflow/#add_module","title":"<code>add_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Adds a child module to the current module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#apply","title":"<code>apply(self:  ~T,  fn:  Callable[[ForwardRef('Module')], None]) -&gt; ~T</code>","text":"<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>). </p> <p></p>"},{"location":"api/style_transfer/styleflow/#bfloat16","title":"<code>bfloat16(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#buffers","title":"<code>buffers(self,  recurse:  bool = True) -&gt; Iterator[torch.Tensor]</code>","text":"<p>Returns an iterator over module buffers. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#children","title":"<code>children(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over immediate children modules. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#compile","title":"<code>compile(self, args, *kwargs)</code>","text":"<p>Compile this Module's forward using :func:<code>torch.compile</code>. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#cpu","title":"<code>cpu(self:  ~T) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the CPU. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#cuda","title":"<code>cuda(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the GPU. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#double","title":"<code>double(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>double</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#eval","title":"<code>eval(self:  ~T) -&gt; ~T</code>","text":"<p>Sets the module in evaluation mode. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#extra_repr","title":"<code>extra_repr(self) -&gt; str</code>","text":"<p>Set the extra representation of the module </p> <p></p>"},{"location":"api/style_transfer/styleflow/#float","title":"<code>float(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>float</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#_forward_unimplemented","title":"<code>_forward_unimplemented(self,  *input:  Any) -&gt; None</code>","text":"<p>Defines the computation performed at every call. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#get_buffer","title":"<code>get_buffer(self,  target:  str) -&gt; 'Tensor'</code>","text":"<p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#get_extra_state","title":"<code>get_extra_state(self) -&gt; Any</code>","text":"<p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#get_parameter","title":"<code>get_parameter(self,  target:  str) -&gt; 'Parameter'</code>","text":"<p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#get_submodule","title":"<code>get_submodule(self,  target:  str) -&gt; 'Module'</code>","text":"<p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#half","title":"<code>half(self:  ~T) -&gt; ~T</code>","text":"<p>Casts all floating point parameters and buffers to <code>half</code> datatype. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#ipu","title":"<code>ipu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the IPU. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#load_state_dict","title":"<code>load_state_dict(self,  state_dict:  Mapping[str, Any],  strict:  bool = True,  assign:  bool = False)</code>","text":"<p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#modules","title":"<code>modules(self) -&gt; Iterator[ForwardRef('Module')]</code>","text":"<p>Returns an iterator over all modules in the network. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#named_buffers","title":"<code>named_buffers(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</code>","text":"<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#named_children","title":"<code>named_children(self) -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</code>","text":"<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#named_modules","title":"<code>named_modules(self,  memo:  Optional[Set[ForwardRef('Module')]] = None,  prefix:  str = '',  remove_duplicate:  bool = True)</code>","text":"<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#named_parameters","title":"<code>named_parameters(self,  prefix:  str = '',  recurse:  bool = True,  remove_duplicate:  bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</code>","text":"<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#parameters","title":"<code>parameters(self,  recurse:  bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</code>","text":"<p>Returns an iterator over module parameters. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_backward_hook","title":"<code>register_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_buffer","title":"<code>register_buffer(self,  name:  str,  tensor:  Optional[torch.Tensor],  persistent:  bool = True) -&gt; None</code>","text":"<p>Adds a buffer to the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_forward_hook","title":"<code>register_forward_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any], Any],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False,  always_call:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_forward_pre_hook","title":"<code>register_forward_pre_hook(self,  hook:  Union[Callable[[~T, Tuple[Any, ...]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable[[~T, Tuple[Any, ...],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dict[str, Any]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Optional[Tuple[Any, Dict[str, Any]]]]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  prepend:  bool = False,  with_kwargs:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a forward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_full_backward_hook","title":"<code>register_full_backward_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward hook on the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_full_backward_pre_hook","title":"<code>register_full_backward_pre_hook(self,  hook:  Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]],\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Union[None, Tuple[torch.Tensor, ...], torch.Tensor]],  prepend:  bool = False) -&gt; torch.utils.hooks.RemovableHandle</code>","text":"<p>Registers a backward pre-hook on the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_load_state_dict_post_hook","title":"<code>register_load_state_dict_post_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>Registers a post hook to be run after module's <code>load_state_dict</code> is called. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_module","title":"<code>register_module(self,  name:  str,  module:  Optional[ForwardRef('Module')]) -&gt; None</code>","text":"<p>Alias for :func:<code>add_module</code>. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_parameter","title":"<code>register_parameter(self,  name:  str,  param:  Optional[torch.nn.parameter.Parameter]) -&gt; None</code>","text":"<p>Adds a parameter to the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#register_state_dict_pre_hook","title":"<code>register_state_dict_pre_hook(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 hook)</code>","text":"<p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#requires_grad_","title":"<code>requires_grad_(self:  ~T,  requires_grad:  bool = True) -&gt; ~T</code>","text":"<p>Change if autograd should record operations on parameters in this module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#set_extra_state","title":"<code>set_extra_state(self,  state:  Any)</code>","text":"<p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#share_memory","title":"<code>share_memory(self:  ~T) -&gt; ~T</code>","text":"<p>See :meth:<code>torch.Tensor.share_memory_</code> </p> <p></p>"},{"location":"api/style_transfer/styleflow/#state_dict","title":"<code>state_dict(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *args,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 destination=None,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 prefix='',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 keep_vars=False)</code>","text":"<p>Returns a dictionary containing references to the whole state of the module. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#style_flow_generate","title":"<code>style_flow_generate(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_image,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_image)</code>","text":"<p>A function that generates one image after training by StyleFlow method. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#to","title":"<code>to(self, args, *kwargs)</code>","text":"<p>Moves and/or casts the parameters and buffers. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#to_empty","title":"<code>to_empty(self:  ~T,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *,  device:  Union[str, torch.device],  recurse:  bool = True) -&gt; ~T</code>","text":"<p>Moves the parameters and buffers to the specified device without copying storage. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#train","title":"<code>train(self:  ~T,  mode:  bool = True) -&gt; ~T</code>","text":"<p>Sets the module in training mode. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#train_network","title":"<code>train_network(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 train_loader,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 content_weight,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 style_weight,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 type_loss=None)</code>","text":"<p>Train the StyleFlow network and return the losses. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#type","title":"<code>type(self:  ~T,  dst_type:  Union[torch.dtype, str]) -&gt; ~T</code>","text":"<p>Casts all parameters and buffers to :attr:<code>dst_type</code>. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#xpu","title":"<code>xpu(self:  ~T,  device:  Union[int, torch.device, None] = None) -&gt; ~T</code>","text":"<p>Moves all model parameters and buffers to the XPU. </p> <p></p>"},{"location":"api/style_transfer/styleflow/#zero_grad","title":"<code>zero_grad(self,  set_to_none:  bool = True) -&gt; None</code>","text":"<p>Resets gradients of all model parameters. See similar function under :class:<code>torch.optim.Optimizer</code> for more context. </p> <p></p> <p>StyleFlow For Content-Fixed Image to Image Translation by Weichen Fan &amp; al (2022).</p>"}]}