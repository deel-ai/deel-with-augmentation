# Cycle GAN

<sub>
    <img src="https://upload.wikimedia.org/wikipedia/commons/d/d0/Google_Colaboratory_SVG_Logo.svg" width="20">
</sub>[View colab tutorial]() |
<sub>
    <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" width="20">
</sub>[View source]() |
ðŸ“° [Paper]()

Have you ever had the dark secret of turning a horse into a zebra? CycleGAN was developed to do just that. Learn how to turn a horse into a zebra and much more.

Back to the story of the dark secret, how are we going to do that?

*`We'll climb over the neighbor's fence in the middle of the night to paint a horse with stripes. Snap a snap of a horse before we start our fucking act. Then paint the horses quickly before their owners notice. Take a photo of the zebra-striped horse's output, then run to the fence before the homeowner's pit bulls spot you. And you'll have to keep doing that over and over again until you have enough example items in your database to train your neural network.`*

Forget it because you have CycleGAN. You will build a generator like the Pix2Pix architecture, which the GAN will train to be a Generator to turn a horse into a zebra. And then you build a Generator (again based on the Pix2Pix architecture) for a second inverse GAN that is supposed to take a picture of a zebra and turn it into an image of a horse.

Let's look at the image below, which shows the first half of CycleGAN trying to create a fake zebra from a horse. The second half of CycleGAN tries to create a fake horse from a zebra. Both halves include loss of cyclic consistency trying to make the output of the inverting generator match the input of the non-inverting generator.

<div class=figure>
  <p align="center" width="100%"> <img width="70%" src="/home/vuong.nguyen/vuong/augmentare/docs/assets/cyclegan1.jpg">
  <p align="center">
</div>

<div class=figure>
  <p align="center" width="100%"> <img width="70%" src="/home/vuong.nguyen/vuong/augmentare/docs/assets/cyclegan2.jpg">
  <p align="center"> Simplified view of CycleGAN architecture <a href="https://hardikbansal.github.io/CycleGANBlog/"> (Image source) </a>
</div>

## NETWORK ARCHITECTURE : CycleGAN

### GENERATOR NETWORK
The CycleGAN Generator has 3 components:

1. A downsampling network: It is composed of 3 convolutional layers  (together with the regular padding, normalization and activation layers).
2. A chain of residual networks built using the Residual Block. You can try to vary the `ResidualBlock` parameter and see the results.
3. A upsampling network: It is composed of 3 transposed convolutional layers.

In CycleGAN Generator, we shall be using **Instance Norm** instead of **Batch Norm** and finally swap the **Zero Padding** of the Convolutional Layer with **Reflection Padding**.

### DISCRIMINATOR NETWORK
The CycleGAN Discriminator is like the standard DCGAN Discriminator. The only difference is the **Instance Normalization** used.

## LOSS FUNCTIONS

The Generator Loss is composed of 3 parts. They are described below:

1. **GAN Loss**: It is the standard generator loss of the Least Squares GAN. We use the functional forms of the losses to implement this part.
$$L_{GAN} = \frac{1}{2} \times ((D_A(G_{B2A}(Image_B)) - 1)^2 + (D_B(G_{A2B}(Image_A)) - 1)^2)$$
2. **Identity Loss**: It computes the similarity of a real image of type B and a fake image B generated from image A and vice versa. The similarity is measured using the $L_1$ Loss.
$$L_{identity} = \frac{1}{2} \times (||G_{B2A}(Image_B) - Image_A||_1 + ||G_{A2B}(Image_A) - Image_B||_1)$$
3. **Cycle Consistency Loss**: This loss computes the similarity of the original image and the image generated by a composition of the 2 generators. This allows cyclegan to deak with unpaired images. We reconstruct the original image and try to minimize the $L_1$ norm between the original images and this reconstructed image.
$$L_{cycle\_consistency} = \frac{1}{2} \times (||G_{B2A}(G_{A2B}(Image_A)) - Image_A||_1 + ||G_{A2B}(G_{B2A}(Image_B)) - Image_B||_1)$$

The Discriminator as mentioned before is same as the normal DCGAN Discriminator. As such even the loss function for that is same as that of the standard GAN:

$$L_{GAN} = \frac{1}{2} \times (((D_A(Image_A) - 1)^2 - (D_A(G_{B2A}(Image_B))^2) + ((D_B(Image_B) - 1)^2 - (D_B(G_{A2B}(Image_A))^2))$$

## Example

```python
# Augmentare Imports
import augmentare
from augmentare.methods.gan import *

# Create GAN Generator
net_gen = CYCLEGANGenerator()

# Create GAN Discriminator
net_dis = CYCLEGANDiscriminator()

# Optimizers and Loss functions
optimizer_gen = Adam(net_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_dis = Adam(net_dis.parameters(), lr=0.0002, betas=(0.5, 0.999))
loss_fn_gen =  nn.L1Loss()
loss_fn_dis =  nn.L1Loss()

# Create GAN network
gan = CYCLEGAN(
    net_gen,
    net_dis,
    optimizer_gen,
    optimizer_dis,
    loss_fn_gen,
    loss_fn_dis,
    device,
    latent_size=None
)

# Training the CycleGAN network
gen_losses, dis_losses = gan.train(
    subset_a=data["A"],
    num_epochs=30,
    num_decay_epochs=15,
    num_classes = None,
    batch_size = None,
    subset_b = data["B"]
)

# Sample images from the Generator
real_a = data["A"][:36]
real_b = data["B"][:36]

fake_image_a, fake_image_b= gan.generate_samples(
    nb_samples = None,
    num_classes = None,
    real_image_a = real_a,
    real_image_b = real_b
)
```

## Notebooks

- [**CycleGAN**: Tutorial]()
- [**CycleGAN**: Apply in CelebA]()

{{augmentare.methods.gan.cyclegan.CYCLEGANGenerator}}

{{augmentare.methods.gan.cyclegan.CYCLEGANDiscriminator}}

{{augmentare.methods.gan.cyclegan.CYCLEGAN}}

[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) by Jun-Yan Zhu & al (2017).
